,blog_title,blog_link,blog_content,blog_questions,blog_subheading,blog_hyperlinks
0,"
Object detection for self-driving cars",https://www.hackerearth.com/blog/developers/object-detection-for-self-driving-cars/,"In the previous blog, Introduction to Object detection, we learned the basics of object detection. We also got an overview of the YOLO (You Look Only Once algorithm). In this blog, we will extend our learning and will dive deeper into the YOLO algorithm. We will learn topics such as intersection over area metrics, non maximal suppression, multiple object detection, anchor boxes, etc. Finally, we will build an object detection detection system for a self-driving car using the YOLO algorithm. We will be using the Berkeley driving dataset to train our model.Before, we get into building the various components of the object detection model, we will perform some preprocessing steps. The preprocessing steps involve resizing the images (according to the input shape accepted by the model) and converting the box coordinates into the appropriate form. Since we will be building a object detection for a self-driving car, we will be detecting and localizing eight different classes. These classes are ‘bike’, ‘bus’, ‘car’, ‘motor’, ‘person’, ‘rider’, ‘train’, and ‘truck’. Therefore, our target variable will be defined as:where,\begin{equation}
\hat{y} ={
\begin{bmatrix}
{p_c}& {b_x} & {b_y} & {b_h} & {b_w} & {c_1} & {c_2} & … & {c_8}
\end{bmatrix}}^T
\end{equation}pc : Probability/confidence of an object being present in the bounding boxbx, by : coordinates of the center of the bounding box bw : width of the bounding box w.r.t the image width bh : height of the bounding box w.r.t the image height ci = Probability of the ith classBut since the box coordinates provided in the dataset are in the following format: xmin, ymin, xmax, ymax (see Fig 1.), we need to convert them according to the target variable defined above. This can be implemented as follows:W : width of the original image
H : height of the original image\begin{equation}
b_x = \frac{(x_{min} + x_{max})}{2 * W}\ , \ b_y = \frac{(y_{min} + y_{max})}{2 * H} \\
b_w = \frac{(x_{max} – x_{min})}{2 * W}\ , \ b_y = \frac{(y_{max} + y_{min})}{2 * W}
\end{equation}Fig 1. Bounding Box Coordinates in the target variableData PreprocessingPython
def process_data(images, boxes=None):
    """"""
    Process the data
    """"""
    images = [PIL.Image.fromarray(i) for i in images]
    orig_size = np.array([images[0].width, images[0].height])
    orig_size = np.expand_dims(orig_size, axis=0)
    
    #Image preprocessing 
    processed_images = [i.resize((416, 416), PIL.Image.BICUBIC) for i in images]
    processed_images = [np.array(image, dtype=np.float) for image in processed_images]
    processed_images = [image/255. for image in processed_images]
    
    if boxes is not None:
        # Box preprocessing
        # Original boxes stored as as 1D list of class, x_min, y_min, x_max, y_max
        boxes = [box.reshape((-1, 5)) for box in boxes]
        # Get extents as y_min, x_min, y_max, x_max, class for comparison with 
        # model output
        box_extents = [box[:, [2,1,4,3,0]] for box in boxes]
        
        # Get box parameters as x_center, y_center, box_width, box_height, class.
        boxes_xy = [0.5* (box[:, 3:5] + box[:, 1:3]) for box in boxes]
        boxes_wh = [box[:, 3:5] - box[:, 1:3] for box in boxes]
        boxes_xy = [box_xy / orig_size for box_xy in boxes_xy]
        boxes_wh = [box_wh / orig_size for box_wh in boxes_wh]
        boxes = [np.concatenate((boxes_xy[i], boxes_wh[i], box[:, 0:1]), axis=-1) for i, box in enumerate(boxes)]
        
        # find the max number of boxes 
        max_boxes = 0
        for boxz in boxes:
            if boxz.shape[0] > max_boxes:
                max_boxes = boxz.shape[0]
        # add zero pad for training 
        for i, boxz in enumerate(boxes):
            if boxz.shape[0] <  max_boxes:
                zero_padding = np.zeros((max_boxes - boxz.shape[0], 5), dtype=np.float32)
                boxes[i] = np.vstack((boxz, zero_padding))
        
        return np.array(processed_images), np.array(boxes)
    else:
        return np.array(processed_images)123456789101112131415161718192021222324252627282930313233343536373839404142def process_data(images, boxes=None):    """"""    Process the data    """"""    images = [PIL.Image.fromarray(i) for i in images]    orig_size = np.array([images[0].width, images[0].height])    orig_size = np.expand_dims(orig_size, axis=0)        #Image preprocessing     processed_images = [i.resize((416, 416), PIL.Image.BICUBIC) for i in images]    processed_images = [np.array(image, dtype=np.float) for image in processed_images]    processed_images = [image/255. for image in processed_images]        if boxes is not None:        # Box preprocessing        # Original boxes stored as as 1D list of class, x_min, y_min, x_max, y_max        boxes = [box.reshape((-1, 5)) for box in boxes]        # Get extents as y_min, x_min, y_max, x_max, class for comparison with         # model output        box_extents = [box[:, [2,1,4,3,0]] for box in boxes]                # Get box parameters as x_center, y_center, box_width, box_height, class.        boxes_xy = [0.5* (box[:, 3:5] + box[:, 1:3]) for box in boxes]        boxes_wh = [box[:, 3:5] - box[:, 1:3] for box in boxes]        boxes_xy = [box_xy / orig_size for box_xy in boxes_xy]        boxes_wh = [box_wh / orig_size for box_wh in boxes_wh]        boxes = [np.concatenate((boxes_xy[i], boxes_wh[i], box[:, 0:1]), axis=-1) for i, box in enumerate(boxes)]                # find the max number of boxes         max_boxes = 0        for boxz in boxes:            if boxz.shape[0] > max_boxes:                max_boxes = boxz.shape[0]        # add zero pad for training         for i, boxz in enumerate(boxes):            if boxz.shape[0] <  max_boxes:                zero_padding = np.zeros((max_boxes - boxz.shape[0], 5), dtype=np.float32)                boxes[i] = np.vstack((boxz, zero_padding))                return np.array(processed_images), np.array(boxes)    else:        return np.array(processed_images)Fig 1. Bounding Box Coordinates in the target variableIntersection over Union (IoU) is an evaluation metric that is used to measure the accuracy of an object detection algorithm. Generally, IoU is a measure of the overlap between two bounding boxes. To calculate this metric, we need:Intersection over Union is the ratio of the area of intersection over the union area occupied by the ground truth bounding box and the predicted bounding box. Fig. 9 shows the IoU calculation for different bounding box scenarios.Intersection over Union is the ratio of the area of intersection over the union area occupied by the ground truth bounding box and the predicted bounding box. Fig. 2 shows the IoU calculation for different bounding box scenarios.Fig 2. Intersection over Union computation for different bounding boxes.Now, that we have a better understanding of the metric, let’s code it.Instead of building the model from scratch, we will be using a pre-trained network and applying transfer learning to create our final model. You only look once (YOLO) is a state-of-the-art, real-time object detection system, which has a mAP on VOC 2007 of 78.6% and a mAP of 48.1% on the COCO test-dev. YOLO applies a single neural network to the full image. This network divides the image into regions and predicts the bounding boxes and probabilities for each region. These bounding boxes are weighted by the predicted probabilities. One of the advantages of YOLO is that it looks at the whole image during the test time, so its predictions are informed by global context in the image. Unlike R-CNN, which requires thousands of networks for a single image, YOLO makes predictions with a single network. This makes this algorithm extremely fast, over 1000x faster than R-CNN and 100x faster than Fast R-CNN.If the target variable $# y $#  is defined as\begin{equation}
y ={
\begin{bmatrix}
{p_c}& {b_x} & {b_y} & {b_h} & {b_w} & {c_1} & {c_2} & {…} & {c_8}
\end{bmatrix}}^T \\
\begin{matrix}
& {y_1}& {y_2} & {y_3} & {y_4} & {y_5} & {y_6} & {y_7} & {…} & {y_{13}}
\end{matrix}
\end{equation}the loss function for object localization is defined as\begin{equation}
\mathcal{L(\hat{y}, y)} =
\begin{cases}
(\hat{y_1} – y_1)^2 + (\hat{y_2} – y_2)^2 + … + (\hat{y_{13}} – y_{13})^2 &&, y_1=1 \\
(\hat{y_1} – y_1)^2 &&, y_1=0
\end{cases}
\end{equation}The loss function in case of the YOLO algorithm is calculated using the following steps:Using the steps defined above, let’s calculate the loss function for the YOLO algorithm.In general, the target variable is defined as\begin{equation}
y ={
\begin{bmatrix}
{p_i(c)}& {x_i} & {y_i} & {h_i} & {w_i} & {C_i}
\end{bmatrix}}^T
\end{equation}where, pi(c) : Probability/confidence of an object being present in the bounding box.
xi, yi : coordinates of the center of the bounding box.
wi : width of the bounding box w.r.t the image width.
hi : height of the bounding box w.r.t the image height.
Ci = Probability of the ith class.then the corresponding loss function is calculated aswhere,   The above equation represents the yolo loss function. The equation may seem daunting at first, but on having a closer look we can see it is the sum of the coordinate loss, the classification loss, and the confidence loss in that order. We use sum of squared errors because it is easy to optimize. However, it weights the localization error equally with classification error which may not be ideal. To remedy this, we increase the loss from bounding box coordinate predictions and decrease the loss from confidence predictions for boxes that don’t contain objects. We use two parameters, λcoord and λnoobj to accomplish this.Note that the loss function only penalizes classification error if an object is present in that grid cell. It also penalizes the bounding box coordinate error if that predictor is responsible for the ground truth box (i.e which has the highest IOU of any predictor in that grid cell).The YOLO model has the following architecture (see Fig 3). The network has 24 convolutional layers followed by two fully connected layers. Alternating 1 × 1 convolutional layers reduce the features space from preceding layers. The convolutional layers are pretrained on the ImageNet classification task at half the resolution (224 × 224 input image) and then double the resolution for detection.Fig 3. The YOLO Archtecture (Image taken from the official YOLO paper)We will be using pre trained YOLOv2 model, which has been trained on the COCO image dataset with classes similar to the Berkeley Driving Dataset. So, we will use the YOLOv2 pretrained network as a feature extractor. We will load the pretrained weights of the YOLOv2 model and will freeze all the weights except for the last layer during training of the model. We will remove the last convolutional layer of the YOLOv2 model and replace it with a new convolutional layer indicating the number of classes (8 classes as defined earlier) to be predicted. This is implemented in the following code.Due to limited computational power, we used only the first 1000 images present in the training dataset to train the model. Finally, we trained the model for 20 epochs and saved the model weights with the lowest loss.The YOLO object detection algorithm will predict multiple overlapping bounding boxes for a given image. As not all bounding boxes contain the object to be classified (e.g. pedestrian, bike, car or truck) or detected, we need to filter out those bounding boxes that don’t contain the target object. To implement this, we monitor the value of pc, i.e., the probability or confidence of an object (i.e. the four classes) being present in the bounding box. If the value of pc is less than the threshold value, then we filter out that bounding box from the predicted bounding boxes. This threshold may vary from model to model and serve as a hyper-parameter for the model.If predicted target variable is defined as:\begin{equation}
\hat{y} ={
\begin{bmatrix}
{p_c}& {b_x} & {b_y} & {b_h} & {b_w} & {c_1} & {c_2} & … & {c_8}
\end{bmatrix}}^T
\end{equation}then discard all bounding boxes where the value of pc < threshold value. The following code implements this approach.Even after filtering by thresholding over the classes score, we may still end up with a lot of overlapping bounding boxes. This is because the YOLO algorithm may detect an object multiple times, which is one of its drawbacks. A second filter called non-maximal suppression (NMS) is used to remove duplicate detections of an object. Non-max suppression uses ‘Intersection over Union’ (IoU) to fix multiple detections.Non-maximal suppression is implemented as follows:In case there are multiple classes/ objects, i.e., if there are four objects/classes, then non-max suppression will run four times, once for every output class.One of the drawbacks of YOLO algorithm is that each grid can only detect one object. What if we want to detect multiple distinct objects in each grid. For example, if two objects or classes are overlapping and share the same grid as shown in the image (see Fig 4.),Fig 4. Two Overlapping bounding boxes with two overlapping classes.We make use of anchor boxes to tackle the issue. Let’s assume the predicted variable is defined as\begin{equation}
\hat{y} ={
\begin{bmatrix}
{p_c}& {b_x} & {b_y} & {b_h} & {b_w} & {c_1} & {c_2} & {…} & {c_8}
\end{bmatrix}}^T
\end{equation}then, we can use two anchor boxes in the following manner to detect two objects in the image simultaneously.Fig 5. Target variable with two bounding boxesEarlier, the target variable was defined such that each object in the training image is assigned to grid cell that contains that object’s midpoint. Now, with two anchor boxes, each object in the training images is assigned to a grid cell that contains the object’s midpoint and anchor box for the grid cell with the highest IOU. So, with the help of two anchor boxes, we can detect at most two objects simultaneously in an image. Fig 6. shows the shape of the final output layer with and without the use of anchor boxes.Fig 6. Shape of the output layer with two anchor boxesAlthough, we can detect multiple images using Anchor boxes, but they still have limitations. For example, if there are two anchor boxes defined in the target variable and the image has three overlapping objects, then the algorithm fails to detect all three objects. Secondly, if two anchor boxes are associated with two objects but have the same midpoint in the box coordinates, then the algorithm fails to differentiate between the objects. Now, that we know the basics of anchor boxes, let’s code it.In the following code we will use 10 anchor boxes. As a result, the algorithm can detect at maximum of 10 objects in a given image.We can combine both the concepts threshold filtering and non-maximal suppression and apply it on the output predicted by the YOLO model. This is implemented in the code below.We will use the trained model to predict the respective classes and the corresponding bounding boxes on a sample of images. The function ‘draw’ runs a tensorflow session and calculates the confidence scores, bounding box coordinates and the output class probabilities for the given sample image. Finally, it computes the xmin, xmax, ymin, ymax from bx,by,bw,bh, scales the bounding boxes according to the input sample image and draws the bounding boxes and class probability for the objects in the input sample image.Fig 7. Sample images with the predicted classes and bounding boxesNext, we will implement the model on a real time video. Since, video is a sequence of images at different time frames, so we will predict the class probabilities and bounding boxes for the image captured at each time frame. We will use OpenCV video capture function to read the video and convert it into image/ frames at different time steps. The video below demonstrates the implementation of the algorithm on a real time video.[Source Code]This brings us to the end of this article. Congratulate yourself on reaching to the end of this blog. As a reward you now have a better understanding of how object detection works (using the YOLO algorithm) and how self driving cars implement this technique to differentiate between cars, trucks, pedestrians, etc. to make better decisions. Finally, I encourage you to implement and play with the code yourself. You can find the full source code related to this article here.Have anything to say? Feel free to comment below for any questions, suggestions, and discussions related to this article. Till then, keep hacking with HackerEarth.Struggling to compose your own music, check out this blog on how to Compose Jazz Music with Deep Learning.",,"Data Preprocessing,,Intersection Over Union,Defining the Model,Tackling Multiple Detection,Object Detection on Sample Test Image,Implementing the Model on Real Time Video,Conclusion,","https://www.hackerearth.com/blog/artificial-intelligence/,https://www.hackerearth.com/blog/cognitive/,https://www.hackerearth.com/blog/data-science/,https://www.hackerearth.com/blog/developers/,https://www.hackerearth.com/blog/machine-learning/,https://www.hackerearth.com/blog/machine-learning/introduction-to-object-detection/,https://www.hackerearth.com/blog/uncategorized/object-detection-for-self-driving-cars/attachment/fig-1-3/,https://www.hackerearth.com/challenges/competitive/on-the-plague-trail-hackerearth-machine-learning-challenge/?utm_source=blog&utm_medium=strip,https://s3-ap-southeast-1.amazonaws.com/he-public-data/Fig%209ba491b7.png,https://s3-ap-southeast-1.amazonaws.com/he-public-data/YOLO%20model317b0cf.PNG,https://s3-ap-southeast-1.amazonaws.com/he-public-data/Loss%20Equation0540788.PNG,https://blog-c7ff.kxcdn.com/blog/wp-content/uploads/2018/09/loss_math_1.png,https://s3-ap-southeast-1.amazonaws.com/he-public-data/YOLO%20model%20architectureeffe104.PNG,https://s3-ap-southeast-1.amazonaws.com/he-public-data/Fig%202-14035d30.png,https://s3-ap-southeast-1.amazonaws.com/he-public-data/Fig%2039a87e41.png,https://s3-ap-southeast-1.amazonaws.com/he-public-data/YOLO%20-%20Object%20Detection1b42b59.png,https://s3-ap-southeast-1.amazonaws.com/he-public-data/Test%20Image%20Output15a821b.jpg,https://github.com/shubham3121/object-detection-using-yolo,https://github.com/shubham3121/object-detection-using-yolo,https://www.hackerearth.com/challenges/,https://www.hackerearth.com/blog/machine-learning/jazz-music-using-deep-learning/,https://pjreddie.com/darknet/yolov2/,https://arxiv.org/abs/1612.08242,https://github.com/allanzelener/YAD2K,http://bdd-data.berkeley.edu/,"
1,"
Introduction to Object Detection",https://www.hackerearth.com/blog/developers/introduction-to-object-detection/,"Humans can easily detect and identify objects present in an image. The human visual system is fast and accurate and can perform complex tasks like identifying multiple objects and detect obstacles with little conscious thought. With the availability of large amounts of data, faster GPUs, and better algorithms, we can now easily train computers to detect and classify multiple objects within an image with high accuracy. In this blog, we will explore terms such as object detection, object localization, loss function for object detection and localization, and finally explore an object detection algorithm known as “You only look once” (YOLO). An image classification or image recognition model simply detect the probability of an object in an image. In contrast to this, object localization refers to identifying the location of an object in the image. An object localization algorithm will output the coordinates of the location of an object with respect to the image. In computer vision, the most popular way to localize an object in an image is to represent its location with the help of bounding boxes. Fig. 1 shows an example of a bounding box.Fig 1. Bounding box representation used for object localizationA bounding box can be initialized using the following parameters:The target variable for a multi-class image classification problem is defined as:\(\hat{y} = {c_i}\)where,
$#\smash{c_i}$# = Probability of the $#i_{th}$# class.
For example, if there are four classes, the target variable is defined as\begin{equation}
y =
\begin{bmatrix}
{c_1} & \\
{c_2} & \\
{c_3} & \\
{c_4}
\end{bmatrix}
\end{equation}
We can extend this approach to define the target variable for object localization. The target variable is defined as
\begin{equation}
y =
\begin{bmatrix}
{p_c} & \\
{b_x} & \\
{b_y} & \\
{b_h} & \\
{b_w} & \\
{c_1} & \\
{c_2} & \\
{c_3} & \\
{c_4}
\end{bmatrix}
\end{equation}
where,
$#\smash{p_c}$# = Probability/confidence of an object (i.e the four classes) being present in the bounding box.
$#\smash{b_x, b_y, b_h, b_w}$# = Bounding box coordinates.
$#\smash{c_i}$# = Probability of the $#\smash{i_{th}}$# class the object belongs to.For example, the four classes be ‘truck’, ‘car’, ‘bike’, ‘pedestrian’ and their probabilities are represented as  $#c_1, c_2, c_3, c_4$#.  So,\begin{equation}
p_c =
\begin{cases}
1,\ \ c_i: \{c_1, c_2, c_3, c_4\} && \\
0,\ \ otherwise
\end{cases}
\end{equation}Let the values of the target variable $#y$# are represented as $#y_1$#, $#y_2$#, $#…,\ y_9$#.\begin{equation}
y ={
\begin{bmatrix}
{p_c}& {b_x} & {b_y} & {b_h} & {b_w} & {c_1} & {c_2} & {c_3} & {c_4}
\end{bmatrix}}^T \\
\begin{matrix}
& {y_1}& {y_2} & {y_3} & {y_4} & {y_5} & {y_6} & {y_7} & {y_8} & {y_9}
\end{matrix}
\end{equation}The loss function for object localization will be defined as\begin{equation}
\mathcal{L(\hat{y}, y)} =
\begin{cases}
(\hat{y_1} – y_1)^2 + (\hat{y_8} – y_8)^2 + … + (\hat{y_9} – y_9)^2 &&, y_1=1 \\
(\hat{y_1} – y_1)^2 &&, y_1=0
\end{cases}
\end{equation}In practice, we can use a log function considering the softmax output in case of the predicted classes ($#c_1, c_2, c_3, c_4$#). While for the bounding box coordinates, we can use something like a squared error and for $#p_c$# (confidence of object) we can use logistic regression loss.Since we have defined both the target variable and the loss function, we can now use neural networks to both classify and localize objects.An approach to building an object detection is to first build a classifier that can classify closely cropped images of an object. Fig 2. shows an example of such a model, where a model is trained on a dataset of closely cropped images of a car and the model predicts the probability of an image being a car.Fig 2. Image classification of carsNow, we can use this model to detect cars using a sliding window mechanism. In a sliding window mechanism, we use a sliding window (similar to the one used in convolutional networks) and crop a part of the image in each slide. The size of the crop is the same as the size of the sliding window. Each cropped image is then passed to a ConvNet model (similar to the one shown in Fig 2.), which in turn predicts the probability of the cropped image is a car.Fig 3. Sliding windows mechanismAfter running the sliding window through the whole image, we resize the sliding window and run it again over the image again. We repeat this process multiple times. Since we crop through a number of images and pass it through the ConvNet, this approach is both computationally expensive and time-consuming, making the whole process really slow. Convolutional implementation of the sliding window helps resolve this problem.Before we discuss the implementation of the sliding window using convents, let’s analyze how we can convert the fully connected layers of the network into convolutional layers. Fig. 4 shows a simple convolutional network with two fully connected layers each of shape (400, ).Fig 4. Sliding windows mechanismA fully connected layer can be converted to a convolutional layer with the help of a 1D convolutional layer. The width and height of this layer are equal to one and the number of filters are equal to the shape of the fully connected layer. An example of this is shown in Fig 5.Fig 5. Converting a fully connected layer into a convolutional layerWe can apply this concept of conversion of a fully connected layer into a convolutional layer to the model by replacing the fully connected layer with a 1-D convolutional layer. The number of the filters of the 1D convolutional layer is equal to the shape of the fully connected layer. This representation is shown in Fig 6. Also, the output softmax layer is also a convolutional layer of shape (1, 1, 4), where 4 is the number of classes to predict.Fig 6. Convolutional representation of fully connected layers.Now, let’s extend the above approach to implement a convolutional version of sliding window. First, let’s consider the ConvNet that we have trained to be in the following representation (no fully connected layers).Let’s assume the size of the input image to be 16 × 16 × 3. If we’re to use a sliding window approach, then we would have passed this image to the above ConvNet four times, where each time the sliding window crops a part of the input image of size 14 × 14 × 3 and pass it through the ConvNet. But instead of this, we feed the full image (with shape 16 × 16 × 3) directly into the trained ConvNet (see Fig. 7). This results in an output matrix of shape 2 × 2 × 4. Each cell in the output matrix represents the result of a possible crop and the classified value of the cropped image. For example, the left cell of the output (the green one) in Fig. 7 represents the result of the first sliding window. The other cells represent the results of the remaining sliding window operations.Fig 7. Convolutional implementation of the sliding windowNote that the stride of the sliding window is decided by the number of filters used in the Max Pool layer. In the example above, the Max Pool layer has two filters, and as a result, the sliding window moves with a stride of two resulting in four possible outputs. The main advantage of using this technique is that the sliding window runs and computes all values simultaneously. Consequently, this technique is really fast. Although a weakness of this technique is that the position of the bounding boxes is not very accurate.A better algorithm that tackles the issue of predicting accurate bounding boxes while using the convolutional sliding window technique is the YOLO algorithm. YOLO stands for you only look once and was developed in 2015 by Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. It’s popular because it achieves high accuracy while running in real time. This algorithm is called so because it requires only one forward propagation pass through the network to make the predictions.The algorithm divides the image into grids and runs the image classification and localization algorithm (discussed under object localization) on each of the grid cells. For example, we have an input image of size 256 × 256. We place a 3 × 3 grid on the image (see Fig. 8).Fig. 8 Grid (3 x 3) representation of the imageNext, we apply the image classification and localization algorithm on each grid cell. For each grid cell, the target variable is defined as\begin{equation}
y_{i, j} ={
\begin{bmatrix}
{p_c}& {b_x} & {b_y} & {b_h} & {b_w} & {c_1} & {c_2} & {c_3} & {c_4}
\end{bmatrix}}^T
\end{equation}Do everything once with the convolution sliding window. Since the shape of the target variable for each grid cell is 1 × 9 and there are 9 (3 × 3) grid cells, the final output of the model will be:The advantages of the YOLO algorithm is that it is very fast and predicts much more accurate bounding boxes. Also, in practice to get more accurate predictions, we use a much finer grid, say 19 × 19, in which case the target output is of the shape 19 × 19 × 9.With this, we come to the end of the introduction to object detection. We now have a better understanding of how we can localize objects while classifying them in an image. We also learned to combine the concept of classification and localization with the convolutional implementation of the sliding window to build an object detection system. In the next blog, we will go deeper into the YOLO algorithm, loss function used, and implement some ideas that make the YOLO algorithm better. Also, we will learn to implement the YOLO algorithm in real time.Have anything to say? Feel free to comment below for any questions, suggestions, and discussions related to this article. Till then, keep hacking with HackerEarth.",,"Object Localization,Object Detection,The YOLO (You Only Look Once) Algorithm,Conclusion,","https://www.hackerearth.com/blog/artificial-intelligence/,https://www.hackerearth.com/blog/cognitive/,https://www.hackerearth.com/blog/developers/,https://www.hackerearth.com/blog/machine-learning/,https://www.hackerearth.com/blog/python/,https://www.hackerearth.com/blog/uncategorized/introduction-to-object-detection/attachment/fig-1-2/,https://www.hackerearth.com/challenges/competitive/on-the-plague-trail-hackerearth-machine-learning-challenge/?utm_source=blog&utm_medium=strip,https://www.hackerearth.com/blog/uncategorized/introduction-to-object-detection/attachment/fig-2-1-2/,https://www.hackerearth.com/blog/uncategorized/introduction-to-object-detection/attachment/fig-3-2/,https://www.hackerearth.com/blog/uncategorized/introduction-to-object-detection/attachment/fig-4-1-2/,https://www.hackerearth.com/blog/uncategorized/introduction-to-object-detection/attachment/fig-5-2/,https://www.hackerearth.com/blog/uncategorized/introduction-to-object-detection/attachment/fig-6-3/,https://www.hackerearth.com/blog/uncategorized/introduction-to-object-detection/attachment/fig-7-3/,https://www.hackerearth.com/blog/uncategorized/introduction-to-object-detection/attachment/fig-8-4/,https://arxiv.org/abs/1506.02640,https://www.hackerearth.com/blog/uncategorized/introduction-to-object-detection/attachment/fig-8-2-2/,https://www.hackerearth.com/blog/uncategorized/introduction-to-object-detection/attachment/fig-8-3-2/,https://www.hackerearth.com/challenges/,"
2,"
Data Visualization for Beginners-Part 3",https://www.hackerearth.com/blog/developers/data-visualization-for-beginners-part-3/,"Bonjour! Welcome to another part of the series on data visualization techniques. In the previous two articles, we discussed different data visualization techniques that can be applied to visualize and gather insights from categorical and continuous variables. You can check out the first two articles here:In this article, we’ll go through the implementation and use of a bunch of data visualization techniques such as heat maps, surface plots, correlation plots, etc. We will also look at different techniques that can be used to visualize unstructured data such as images, text, etc.A heat map(or heatmap) is a two-dimensional graphical representation of the data which uses colour to represent data points on the graph. It is useful in understanding underlying relationships between data values that would be much harder to understand if presented numerically in a table/ matrix.Fig 1. Heatmap using the seaborn libraryFig 1. Heatmap using the seaborn libraryLet’s understand this using an example. We’ll be using the metadata from Deep Learning 3 challenge. Link to the dataset. Deep Learning 3 challenged the participants to predict the attributes of animals by looking at their images.We will be analyzing how often an attribute occurs in relationship with the other attributes. To analyze this relationship, we will compute the co-occurrence matrix.We can see that the values in the co-occurrence matrix represent the occurrence of each attribute with the other attributes. Although the matrix contains all the information, it is visually hard to interpret and infer from the matrix. To counter this problem, we will use heat maps, which can help relate the co-occurrences graphically.Fig 2. Heatmap of the co-occurrence matrix indicating the frequency of occurrence of one attribute with otherFig 2. Heatmap of the co-occurrence matrix indicating the frequency of occurrence of one attribute with otherSince the frequency of the co-occurrence is represented by a colour pallet, we can now easily interpret which attributes appear together the most. Thus, we can infer that these attributes are common to most of the animals.Choropleths are a type of map that provides an easy way to show how some quantity varies across a geographical area or show the level of variability within a region. A heat map is similar but doesn’t include geographical boundaries. Choropleth maps are also appropriate for indicating differences in the distribution of the data over an area, like ownership or use of land or type of forest cover, density information, etc. We will be using the geopandas library to implement the choropleth graph.We will be using choropleth graph to visualize the GDP across the globe. Link to the dataset.Fig 3. Choropleth graph indicating the GDP according to geographical locationsFig 3. Choropleth graph indicating the GDP according to geographical locationsSurface plots are used for the three-dimensional representation of the data. Rather than showing individual data points, surface plots show a functional relationship between a dependent variable (Z) and two independent variables (X and Y).It is useful in analyzing relationships between the dependent and the independent variables and thus helps in establishing desirable responses and operating conditions.One of the main applications of surface plots in machine learning or data science is the analysis of the loss function. From a surface plot, we can analyze how the hyperparameters affect the loss function and thus help prevent overfitting of the model.Fig 4. Surface plot visualizing the dependent variable w.r.t the independent variables in 3-dimensionsDimensionality refers to the number of attributes present in the dataset. For example, consumer-retail datasets can have a vast amount of variables (e.g. sales, promos, products, open, etc.). As a result, visually exploring the dataset to find potential correlations between variables becomes extremely challenging.Therefore, we use a technique called dimensionality reduction to visualize higher dimensional datasets. Here, we will focus on two such techniques :Before we jump into understanding PCA, let’s review some terms:A positive covariance means X and Y are positively related, i.e., if X increases, Y increases, while negative covariance means the opposite relation. However, zero variance means X and Y are not related.Fig 5. Different types of covariancePCA is the orthogonal projection of data onto a lower-dimension linear space that maximizes variance (green line) of the projected data and minimizes the mean squared distance between the data point and the projects (blue line). The variance describes the direction of maximum information while the mean squared distance describes the information lost during projection of the data onto the lower dimension.Thus, given a set of data points in a d-dimensional space, PCA projects these points onto a lower dimensional space while preserving as much information as possible.Fig 6. Illustration of principal component analysisIn the figure, the component along the direction of maximum variance is defined as the first principal axis. Similarly, the component along the direction of second maximum variance is defined as the second principal component, and so on. These principal components are referred to the new dimensions carrying the maximum information.We can see that 98% (approx) variance of the data is along the first principal component, while the second component only expresses 1.6% (approx) of the data.Fig 7. Visualizing the distribution of cancer across the dataFig 7. Visualizing the distribution of cancer across the dataThus, with the help of PCA, we can get a visual perception of how the labels are distributed across given data (see Figure).T-distributed Stochastic Neighbour Embeddings (t-SNE) is a non-linear dimensionality reduction technique that is well suited for visualization of high-dimensional data. It was developed by Laurens van der Maten and Geoffrey Hinton. In contrast to PCA, which is a mathematical technique, t-SNE adopts a probabilistic approach.PCA can be used for capturing the global structure of the high-dimensional data but fails to describe the local structure within the data. Whereas, “t-SNE” is capable of capturing the local structure of the high-dimensional data very well while also revealing global structure such as the presence of clusters at several scales. t-SNE converts the similarity between data points to joint probabilities and tries to maximize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embeddings and high-dimension data. In doing so, it preserves the original structure of the data.Fig 8. Visualizing the feature space of the iris dataset using t-SNEFig 8. Visualizing the feature space of the iris dataset using t-SNEThus, by reducing the dimensions using t-SNE, we can visualize the distribution of the labels over the feature space. We can see that in the figure the labels are clustered in their own little group. So, if we’re to use a clustering algorithm to generate clusters using the new features/components, we can accurately assign new points to a label. Let’s quickly summarize the topics we covered. We started with the generation of heatmaps using random numbers and extended its application to a real-world example. Next, we implemented choropleth graphs to visualize the data points with respect to geographical locations. We moved on to implement surface plots to get an idea of how we can visualize the data in a three-dimensional surface. Finally, we used two- dimensional reduction techniques, PCA and t-SNE, to visualize high-dimensional datasets.I encourage you to implement the examples described in this article to get a hands-on experience. Hope you enjoyed the article. Do let me know if you have any feedback, suggestions, or thoughts on this article in the comments below! ",,"Heatmaps,Choropleth,Surface plot,Visualizing high-dimensional datasets,Conclusion,","https://www.hackerearth.com/blog/big-data/,https://www.hackerearth.com/blog/data-science/,https://www.hackerearth.com/blog/developers/,https://www.hackerearth.com/blog/machine-learning/,https://www.hackerearth.com/blog/python/,https://www.hackerearth.com/blog/machine-learning/data-visualization-techniques/,https://www.hackerearth.com/blog/machine-learning/data-visualization-for-beginners-part-2/,https://www.hackerearth.com/blog/uncategorized/data-visualization-techniques-part-3/attachment/heatmap1/,https://s3-ap-southeast-1.amazonaws.com/he-public-data/DL3%20Datasete37c45e.torrent,https://www.hackerearth.com/blog/uncategorized/data-visualization-techniques-part-3/attachment/heatmap2/,https://www.hackerearth.com/challenges/competitive/on-the-plague-trail-hackerearth-machine-learning-challenge/?utm_source=blog&utm_medium=strip,https://s3-ap-southeast-1.amazonaws.com/he-public-data/GDPa3d91cf.csv,https://www.hackerearth.com/blog/uncategorized/data-visualization-techniques-part-3/attachment/choropleth/,https://www.hackerearth.com/blog/uncategorized/data-visualization-techniques-part-3/attachment/surface-plot/,https://blog-c7ff.kxcdn.com/blog/wp-content/uploads/2018/07/Variance.png,https://blog-c7ff.kxcdn.com/blog/wp-content/uploads/2018/07/covariance-eq.png,https://www.hackerearth.com/blog/uncategorized/data-visualization-techniques-part-3/attachment/covariance/,https://www.hackerearth.com/blog/uncategorized/data-visualization-techniques-part-3/attachment/pca/,https://www.hackerearth.com/blog/uncategorized/data-visualization-techniques-part-3/attachment/pca2/,https://www.hackerearth.com/blog/uncategorized/data-visualization-techniques-part-3/attachment/tsne/,"
3,"
Composing Jazz Music with Deep Learning",https://www.hackerearth.com/blog/developers/jazz-music-using-deep-learning/,"Deep Learning is on the rise, extending its application in every field, ranging from computer vision to natural language processing, healthcare, speech recognition, generating art, addition of sound to silent movies, machine translation, advertising, self-driving cars, etc. In this blog, we will extend the power of deep learning to the domain of music production. We will talk about how we can use deep learning to generate new musical beats.The current technological advancements have transformed the way we produce music, listen, and work with music. With the advent of deep learning, it has now become possible to generate music without the need for working with instruments artists may not have had access to or the skills to use previously. This offers artists more creative freedom and ability to explore different domains of music.Since music is a sequence of notes and chords, it doesn’t have a fixed dimensionality. Traditional deep neural network techniques cannot be applied to generate music as they assume the inputs and targets/outputs to have fixed dimensionality and outputs to be independent of each other. It is therefore clear that a domain-independent method that learns to map sequences to sequences would be useful.Recurrent neural networks (RNNs) are a class of artificial neural networks that make use of sequential information present in the data. Fig. 1 A basic RNN unit.A recurrent neural network has looped, or recurrent, connections which allow the network to hold information across inputs. These connections can be thought of as memory cells. In other words, RNNs can make use of information learned in the previous time step. As seen in Fig. 1, the output of the previous hidden/activation layer is fed into the next hidden layer. Such an architecture is efficient in learning sequence-based data.In this blog, we will be using the Long Short-Term Memory (LSTM) architecture. LSTM is a type of recurrent neural network (proposed by Hochreiter and Schmidhuber, 1997) that can remember a piece of information and keep it saved for many timesteps.Our dataset includes piano tunes stored in the MIDI format. MIDI (Musical Instrument Digital Interface) is a protocol which allows electronic instruments and other digital musical tools to communicate with each other. Since a MIDI file only represents player information, i.e., a series of messages like ‘note on’, ‘note off, it is more compact, easy to modify, and can be adapted to any instrument.Before we move forward, let us understand some music related terminologies:We will use the music21 toolkit (a toolkit for computer-aided musicology, MIT) to extract data from these MIDI files.The function get_notes returns a list of notes and chords present in the .mid file. We use the converter.parse function to convert the midi file in a stream object, which in turn is used to extract notes and chords present in the file. The list returned by the function get_notes() looks as follows:We can see that the list consists of pitches and chords (represented as a list of integers separated by a dot). We assume each new chord to be a new pitch on the list. As letters are used to generate words in a sentence, similarly the music vocabulary used to generate music is defined by the unique pitches in the notes list.A neural network accepts only real values as input and since the pitches in the notes list are in string format, we need to map each pitch in the notes list to an integer. We can do so as follows:Next, we will create an array of input and output sequences to train our model. Each input sequence will consist of 100 notes, while the output array stores the 101st note for the corresponding input sequence. So, the objective of the model will be to predict the 101st note of the input sequence of notes.Next, we reshape and normalize the input vector sequence before feeding it to the model. Finally, we one-hot encode our output vector.We will use keras to build our model architecture. We use a character level-based architecture to train the model. So each input note in the music file is used to predict the next note in the file, i.e., each LSTM cell takes the previous layer activation (a⟨t−1⟩) and the previous layers actual output (y⟨t−1⟩) as input at the current time step tt. This is depicted in the following figure (Fig 2.).Fig 2. One to Many LSTM architectureOur model architecture is defined as:Our music model consists of two LSTM layers with each layer consisting of 128 hidden layers. We use ‘categorical cross entropy‘ as the loss function and ‘adam‘ as the optimizer. Fig. 3 shows the model summary.Fig 3. Model summaryTo train the model, we call the model.fit function with the input and output sequences as the input to the function. We also create a model checkpoint which saves the best model weights.The train_network method gets the notes, creates the input and output sequences, creates a model, and trains the model for 200 epochs.Now that we have trained our model, we can use it to generate some new notes. To generate new notes, we need a starting note. So, we randomly pick an integer and pick a random sequence from the input sequence as a starting point.Next, we use the trained model to predict the next 500 notes. At each time step, the output of the previous layer (ŷ⟨t−1⟩) is provided as input (x⟨t⟩) to the LSTM layer at the current time step t. This is depicted in the following figure (see Fig. 4).Fig 4. Sampling from a trained network.Since the predicted output is an array of probabilities, we choose the output at the index with the maximum probability. Finally, we map this index to the actual note and add this to the list of predicted output. Since the predicted output is a list of strings of notes and chords, we cannot play it. Hence, we encode the predicted output into the MIDI format using the create_midi method.To create some new jazz music, you can simply call the generate() method, which calls all the related methods and saves the predicted output as a MIDI file.To play the generated MIDI in the Jupyter Notebook you can import the play_midi method from the play.py file or use an external MIDI player or convert the MIDI file to the mp3. Let’s listen to our generated jazz piano music.

Generated Track 1
 {""type"":""audio"",""tracklist"":true,""tracknumbers"":true,""images"":false,""artists"":true,""tracks"":[{""src"":""https:\/\/www.hackerearth.com\/blog\/wp-content\/uploads\/2018\/05\/media-080a21d1.mp3"",""type"":""audio\/mpeg"",""title"":""Generated Track 1"",""caption"":""Generated Sample Music"",""description"":""Generated Track 1"",""meta"":{""artist"":""Recurrent Neural Network"",""album"":""Deep Learning"",""length_formatted"":""2:07""},""image"":{""src"":""https:\/\/www.hackerearth.com\/blog\/wp-includes\/images\/media\/audio.png"",""width"":48,""height"":64},""thumb"":{""src"":""https:\/\/www.hackerearth.com\/blog\/wp-includes\/images\/media\/audio.png"",""width"":48,""height"":64}}]} Congratulations! You can now generate your own jazz music. You can find the full code in this Github repository. I encourage you to play with the parameters of the model and train the model with input sequences of different sequence lengths. Try to implement the code for some other instrument (such as guitar). Furthermore, such a character-based model can also be applied to a text corpus to generate sample texts, such as a poem.Also, you can showcase your own personal composer and any similar idea in the World Music Hackathon by HackerEarth.Have anything to say? Feel free to comment below for any questions, suggestions, and discussions related to this article. Till then, happy coding.",,"Recurrent Neural Networks,Dataset,Data Preprocessing,Model Architecture,Model Training,Music Sample Generation,","https://www.hackerearth.com/blog/artificial-intelligence/,https://www.hackerearth.com/blog/data-science/,https://www.hackerearth.com/blog/developers/,https://www.hackerearth.com/blog/machine-learning/,https://www.hackerearth.com/blog/uncategorized/generate-music-using-deep-learning/attachment/rnn/,http://web.mit.edu/music21/doc/moduleReference/moduleConverter.html,https://www.hackerearth.com/challenges/competitive/on-the-plague-trail-hackerearth-machine-learning-challenge/?utm_source=blog&utm_medium=strip,https://www.hackerearth.com/blog/uncategorized/generate-music-using-deep-learning/attachment/music_generation/,https://www.hackerearth.com/blog/uncategorized/generate-music-using-deep-learning/attachment/model-summary/,https://www.hackerearth.com/blog/uncategorized/generate-music-using-deep-learning/attachment/music_sampling/,https://blog-c7ff.kxcdn.com/blog/wp-content/uploads/2018/05/media-080a21d1.mp3,https://github.com/shubham3121/music-generation-using-rnn,https://goo.gl/LHbbP7,"
4,"
Data visualization for beginners – Part 2",https://www.hackerearth.com/blog/developers/data-visualization-for-beginners-part-2/,"Welcome to Part II of the series on data visualization. In the last blog post, we explored different ways to visualize continuous variables and infer information. If you haven’t visited that article, you can find it here. In this blog, we will expand our exploration to categorical variables and investigate ways in which we can visualize and gain insights from them, in isolation and in combination with variables (both categorical and continuous).Before we dive into the different graphs and plots, let’s define a categorical variable. In statistics, a categorical variable is one which has two or more categories, but there is no intrinsic ordering to them, for example, gender, color, cities, age group, etc. If there is some kind of ordering between the categories, the variables are classified as ordinal variables, for example, if you categorize car prices by cheap, moderate and expensive. Although these are categories, there is a clear ordering between the categories.We will be using the Adult data set, which is an extraction of the 1994 census dataset. The prediction task is to determine whether a person makes more than 50K a year. Here is the link to the dataset. In this blog, we will be using the dataset only for data analysis.A bar chart or graph is a graph with rectangular bars or bins that are used to plot categorical values. Each bar in the graph represents a categorical variable and the height of the bar is proportional to the value represented by it.Bar graphs are used:Fig 1. Bar plot showing the distribution of gender in the datasetFig 1. Bar plot showing the distribution of gender in the datasetFrom the figure, we can infer that there are more number of males than females in the dataset. Next, we will use the bar graph to visualize the distribution of annual income based on both gender and hours per week (i.e. the number of hours they work per week). So from the figure above, we can infer that males and females with annual income less than 50K tend to work more per week.This is a seaborn-specific function which is used to plot the count or frequency distribution of each unique observation in the categorical variable. It is similar to a histogram over a categorical rather than quantitative variable.So, let’s plot the number of males and females in the dataset using the countplot function.Fig 3. Distribution of gender using countplot.Fig 3. Distribution of gender using countplot.Earlier, we plotted the same thing using a bar graph, and it required some external calculations on our part to do so. But we can do the same thing using the countplot function in just a single line of code. Next, we will see how we can use countplot for deeper insights.Fig 4. Distribution of gender based on annual income using countplot.Fig 4. Distribution of gender based on annual income using countplot.From the figure above, we can count that number of males and females whose annual income is <=50 and > 50K. We can see that the approximate number ofSo, we can infer that out of 32,500 (approx) people, only 8000 people have income greater than 50K, out of which only 1000 of them are females.Box plots are widely used in data visualization. Box plots, also known as box and whisker plots are used to visualize variations and compare different categories in a given set of data. It doesn’t display the distribution in detail but is useful in detecting whether a distribution is skewed and detect outliers in the data. In a box and whisker plot:Fig 5. Box and whisker plot.Let’s use a box and whisker plot to find a correlation between ‘hours-per-week’ and ‘relationship’ based on their annual income.Fig 6. Using box plot to visualize how people in different relationships earn based on the number of hours they work per week.Fig 6. Using box plot to visualize how people in different relationships earn based on the number of hours they work per week.We can interpret some interesting results from the box plot. People with the same relationship status and an annual income more than 50K often work for more hours per week. Similarly, we can also infer that people who have a child and earn less than 50K tend to have more flexible working hours.
Apart from this, we can also detect outliers in the data. For example, people with relationship status ‘Not in family’ (see Fig 6.) and an income less than 50K have a large number of outliers at both the high and low ends. This also seems to be logically correct as a person who earns less than 50K annually may work more or less depending on the type of job and employment status.Strip plot is a data analysis technique used to plot the sorted values of a variable along one axis. It is used to represent the distribution of a continuous variable with respect to the different levels of a categorical variable. For example, a strip plot can be used to show the distribution of the variable ‘gender’, i.e., males and females, with respect to the number of hours they work each week. A strip plot is also a good complement to a box plot or a violin plot in cases where you want to showcase all the observations along with some representation of the underlying distribution.Fig 7. Strip plot showing the distribution of the earnings based on the number of hours they work per week.Fig 7. Strip plot showing the distribution of the earnings based on the number of hours they work per week.In the figure, by looking at the distribution of the data points, we can deduce that most of the people with an annual income greater than 50K work between 40 and 60 hours per week. While those with income less than 50K work can work between 0 and 60 hours per week.Sometimes the mean and median may not be enough to understand the distribution of the variable in the dataset. The data may be clustered around the maximum or minimum with nothing in the middle. Box plots are a great way to summarize the statistical information related to the distribution of the data (through the interquartile range, mean, median), but they cannot be used to visualize the variations in the distributions.A violin plot is a combination of a box plot and kernel density function (KDE, described in Part I of this blog series) which can be used to visualize the probability distribution of the data. Violin plots can be interpreted as follows:Fig 8. Representation of a violin plot.Fig 8. Representation of a violin plot.Let’s now build a violin plot. To start with, we will analyze the distribution of annual income of the people w.r.t. the number of hours they work per week.Fig 9. Violin plot showing the distribution of the annual income based on the number of hours they work per week.Fig 9. Violin plot showing the distribution of the annual income based on the number of hours they work per week.In Fig 9, the median number working hours per week is same (40 approximately) for both people earning less than 50K and greater than 50K. Although people earning less than 50K can have a varied range of the hours they spend working per week, most of the people who earn more than 50K work in the range of 40 – 80 hours per week.Next, we can visualize the same distribution, but this grouping them according to their gender.Fig 10. Distribution of annual income based on the number of hours worked per week and gender.Fig 10. Distribution of annual income based on the number of hours worked per week and gender.Adding the variable ‘gender’, gives us insights into how much each gender spends working per week based upon their annual income. From the figure, we can infer that males with annual income less than 50K tends to spend more hours working per week than females. But for people earning greater than 50K, both males and females spend an equal amount of hours per week working.Violin plots, although more informative, are less frequently used in data visualization. It may be because they are hard to grasp and understand at first glance. But their ability to represent the variations in the data are making them popular among machine learning and data enthusiasts.PairGrid is used to plot the pairwise relationship of all the variables in a dataset. This may seem to be similar to the pairplot we discussed in part I of this series. The difference is that instead of plotting all the plots automatically, as in the case of pairplot, Pair Grid creates a class instance, allowing us to map specific functions to the different sections of the grid.Let’s start by defining the class.The variable ‘g’ here is a class instance. If we were to display ‘g’, then we will get a grid of empty plots. There are four grid sections to fill in a Pair Grid: upper triangle, lower triangle, the diagonal, and off-diagonal. To fill all the sections with the same plot, we can simply call ‘g.map’ with the type of plot and plot parameters.Fig 11. Scatter plot between each variable pair in the dataset.Fig 11. Scatter plot between each variable pair in the dataset.The ‘g.map_lower’ method only fills the lower triangle of the grid while the ‘g.map_upper’ method only fills the upper triangle of the grid. Similarly, ‘g.map_diag’ and ‘g.map_offdiag’ fills the diagonal and off-diagonal of the grid, respectively.Fig 12. Pair Grid showing different plot between the different pair of variables.Fig 12. Pair Grid showing different plot between the different pair of variables.Thus with the help of Pair Grid, we can visualize the relationship between the three variables (‘hours-per-week’, ‘education-num’ and ‘age’) using three different plots all in the same figure. Pair grid comes in handy when visualizing multiple plots in the same figure.Let’s summarize what we learned. So, we started with visualizing the distribution of categorical variables in isolation. Then, we moved on to visualize the relationship between a categorical and a continuous variable. Finally, we explored visualizing relationships when more than two variables are involved. Next week, we will explore how we can visualize unstructured data. Finally, I encourage you to download the given census data (used in this blog) or any other dataset of your choice and play with all the variations of the plots learned in this blog. Till then, Adiós!",,"Bar graph,Countplot,Box plot,Strip plot,Violin plot,PairGrid,Conclusion,","https://www.hackerearth.com/blog/big-data/,https://www.hackerearth.com/blog/data-science/,https://www.hackerearth.com/blog/developers/,https://www.hackerearth.com/blog/machine-learning/,https://www.hackerearth.com/blog/python/,https://www.hackerearth.com/blog/machine-learning/data-visualization-techniques/,https://archive.ics.uci.edu/ml/datasets/adult,https://blog-c7ff.kxcdn.com/blog/wp-content/uploads/2018/05/Fig-1.png,https://blog-c7ff.kxcdn.com/blog/wp-content/uploads/2018/05/Fig-3.png,https://blog-c7ff.kxcdn.com/blog/wp-content/uploads/2018/05/Fig-4.png,https://www.hackerearth.com/challenges/competitive/on-the-plague-trail-hackerearth-machine-learning-challenge/?utm_source=blog&utm_medium=strip,https://blog-c7ff.kxcdn.com/blog/wp-content/uploads/2018/05/image_1.jpg,https://blog-c7ff.kxcdn.com/blog/wp-content/uploads/2018/05/Fig-6.png,https://blog-c7ff.kxcdn.com/blog/wp-content/uploads/2018/05/Fig-7.png,https://blog-c7ff.kxcdn.com/blog/wp-content/uploads/2018/05/Fig-8.jpg,https://blog-c7ff.kxcdn.com/blog/wp-content/uploads/2018/05/Fig-9.png,https://blog-c7ff.kxcdn.com/blog/wp-content/uploads/2018/05/Fig-10.png,https://blog-c7ff.kxcdn.com/blog/wp-content/uploads/2018/05/Fig-11.png,https://blog-c7ff.kxcdn.com/blog/wp-content/uploads/2018/05/Fig-12.png,"
5,"
Data visualization for beginners – Part 1",https://www.hackerearth.com/blog/developers/data-visualization-techniques/,"This is a series of blogs dedicated to different data visualization techniques used in various domains of machine learning. Data Visualization is a critical step for building a powerful and efficient machine learning model. It helps us to better understand the data, generate better insights for feature engineering, and, finally, make better decisions during modeling and training of the model.For this blog, we will use the seaborn and matplotlib libraries to generate the visualizations. Matplotlib is a MATLAB-like plotting framework in python, while seaborn is a python visualization library based on matplotlib. It provides a high-level interface for producing statistical graphics. In this blog, we will explore different statistical graphical techniques that can help us in effectively interpreting and understanding the data. Although all the plots using the seaborn library can be built using the matplotlib library, we usually prefer the seaborn library because of its ability to handle DataFrames.We will start by importing the two libraries. Here is the guide to installing the matplotlib library and seaborn library. (Note that I’ll be using matplotlib and seaborn libraries interchangeably depending on the plot.)Let’s begin by plotting a simple line plot which is used to plot a mathematical. A line plot is used to plot the relationship or dependence of one variable on another. Say, we have two variables ‘x’ and ‘y’ with the following values:To plot the relationship between the two variables, we can simply call the plot function.Fig. 1. Line Plot between X and YFig. 1. Line Plot between X and YHere, we can see that the variables ‘x’ and ‘y’ have a sinusoidal relationship. Generally, .plot() function is used to find any mathematical relationship between the variables.A histogram is one of the most frequently used data visualization techniques in machine learning. It represents the distribution of a continuous variable over a given interval or period of time. Histograms plot the data by dividing it into intervals called ‘bins’. It is used to inspect the underlying frequency distribution (eg. Normal distribution), outliers, skewness, etc.Let’s assume some data ‘x’ and analyze its distribution and other related features.Let’s plot a histogram to analyze the distribution of ‘x’.Fig 2. Histogram showing the distribution of the variable ‘x’.Fig 2. Histogram showing the distribution of the variable ‘x’.The above plot shows a normal distribution, i.e., the variable ‘x’ is normally distributed. We can also infer that the distribution is somewhat negatively skewed. We usually control the ‘bins’ parameters to produce a distribution with smooth boundaries. For example, if we set the number of ‘bins’ too low, say bins=5, then most of the values get accumulated in the same interval, and as a result they produce a distribution which is hard to predict.Fig 3. Histogram with low number of bins.Fig 3. Histogram with low number of bins.Similarly, if we increase the number of ‘bins’ to a high value, say bins=1000, each value will act as a separate bin, and as a result the distribution seems to be too random.Fig. 4. Histogram with a large number of bins.Fig. 4. Histogram with a large number of bins.Before we dive into understanding KDE, let’s understand what parametric and non-parametric data are.Parametric Data: When the data is assumed to have been drawn from a particular distribution and some parametric test can be applied to itNon-Parametric Data: When we have no knowledge about the population and the underlying distributionKernel Density Function is the non-parametric way of representing the probability distribution function of a random variable. It is used when the parametric distribution of the data doesn’t make much sense, and you want to avoid making assumptions about the data.The kernel density estimator is the estimated pdf of a random variable. It is defined as

Similar to histograms, KDE plots the density of observations on one axis with height along the other axis.Fig 5. KDE plot for the random variable ‘x’.Fig 5. KDE plot for the random variable ‘x’.Distplot combines the function of the histogram and the KDE plot into one figure.Fig 6. Displot for the random variable ‘x’.Fig 6. Displot for the random variable ‘x’.So, the distplot function plots the histogram and the KDE for the sample data in the same figure. You can tune the parameters of the displot to only display the histogram or kde or both. Distplot comes in handy when you want to visualize how close your assumption about the distribution of the data is to the actual distribution.Scatter plots are used to determine the relationship between two variables. They show how much one variable is affected by another. It is the most commonly used data visualization technique and helps in drawing useful insights when comparing two variables. The relationship between two variables is called correlation. If the data points fit a line or curve with a positive slope, then the two variables are said to show positive correlation. If the line or curve has a negative slope, then the variables are said to have a negative correlation.A perfect positive correlation has a value of 1 and a perfect negative correlation has a value of -1. The closer the value is to 1 or -1, the stronger the relationship between the variables. The closer the value is to 0, the weaker the correlation.For our example, let’s define three variables ‘x’, ‘y’, and ‘z’, where ‘x’ and ‘z’ are randomly generated data and ‘y’ is defined as
We will use a scatter plot to find the relationship between the variables ‘x’ and ‘y’.Fig 7. Scatter plot between X and Y.Fig 7. Scatter plot between X and Y.From the figure above we can see that the data points are very close to each other and also if we fit a curve, along with the points, it will have a positive slope. Therefore, we can infer that there is a strong positive correlation between the values of the variable ‘x’ and variable ‘y’.Also, we can see that the curve that best fits the graph is quadratic in nature and this can be confirmed by looking at the definition of the variable ‘y’.Jointplot is seaborn library specific and can be used to quickly visualize and analyze the relationship between two variables and describe their individual distributions on the same plot.Let’s start with using joint plot for producing the scatter plot.Fig 8. Joint plot (scatter plot) between X and Y.Fig 8. Joint plot (scatter plot) between X and Y.Next, we can use the joint point to find the best line or curve that fits the plot.Fig 9. Using joint plot to plot the regression line that best fits the data points.Fig 9. Using joint plot to plot the regression line that best fits the data points.Apart from this, jointplot can also be used to plot ‘kde’, ‘hex plot’, and ‘residual plot’.We can use scatter plot to plot the relationship between two variables. But what if the dataset has more than two variables (which is quite often the case), it can be a tedious task to visualize the relationship between each variable with the other variables.The seaborn pairplot function does the same thing for us and in just one line of code. It is used to plot multiple pairwise bivariate (two variable) distribution in a dataset. It creates a matrix and plots the relationship for each pair of columns. It also draws a univariate distribution for each variable on the diagonal axes.Sklearn stores data in the form of a numpy array and not data frames, thereby storing the data in a dataframe.Fig 10. Pair plot showing the relationships between the columns of the dataset.Fig 10. Pair plot showing the relationships between the columns of the dataset.So, in the graph above, we can see the relationships between each of the variables with the other and thus infer which variables are most correlated.Visualizations play an important role in data analysis and exploration. In this blog, we got introduced to different kinds of plots used for data analysis of continuous variables. Next week, we will explore the various data visualization techniques that can be applied to categorical variables or variables with discrete values. Next, I encourage you to download the iris dataset or any other dataset of your choice and apply and explore the techniques learned in this blog.Have anything to say? Feel free to comment below for any questions, suggestions, and discussions related to this article. Till then, Sayōnara.",,"Simple Plot,Histogram,Kernel Density Function,Scatter Plot,Joint Plot,PairPlot,Conclusion,","https://www.hackerearth.com/blog/big-data/,https://www.hackerearth.com/blog/data-science/,https://www.hackerearth.com/blog/developers/,https://www.hackerearth.com/blog/machine-learning/,https://www.hackerearth.com/blog/python/,https://blog-c7ff.kxcdn.com/blog/wp-content/uploads/2018/05/fig1.png,https://www.hackerearth.com/challenges/competitive/on-the-plague-trail-hackerearth-machine-learning-challenge/?utm_source=blog&utm_medium=strip,https://blog-c7ff.kxcdn.com/blog/wp-content/uploads/2018/05/fig2.png,https://blog-c7ff.kxcdn.com/blog/wp-content/uploads/2018/05/fig3.png,https://blog-c7ff.kxcdn.com/blog/wp-content/uploads/2018/05/fig4.png,https://blog-c7ff.kxcdn.com/blog/wp-content/uploads/2018/05/Eq1.png,https://blog-c7ff.kxcdn.com/blog/wp-content/uploads/2018/05/fig5.png,https://blog-c7ff.kxcdn.com/blog/wp-content/uploads/2018/05/fig6.png,https://blog-c7ff.kxcdn.com/blog/wp-content/uploads/2018/05/Eq2.png,https://blog-c7ff.kxcdn.com/blog/wp-content/uploads/2018/05/fig7.png,https://blog-c7ff.kxcdn.com/blog/wp-content/uploads/2018/05/fig8.png,https://blog-c7ff.kxcdn.com/blog/wp-content/uploads/2018/05/fig9.png,https://blog-c7ff.kxcdn.com/blog/wp-content/uploads/2018/05/fig10.png,"
6,"
11 open source frameworks for AI and machine learning models",https://www.hackerearth.com/blog/developers/11-open-source-frameworks-ai-machine-learning-models/,"The meteoric rise of artificial intelligence in the last decade has spurred a huge demand for AI and ML skills in today’s job market. ML-based technology is now used in almost every industry vertical from finance to healthcare. In this blog, we have compiled a list of best frameworks and libraries that you can use to build machine learning models.1) TensorFlow
Developed by Google, TensorFlow is an open-source software library built for deep learning or artificial neural networks. With TensorFlow, you can create neural networks and computation models using flowgraphs. It is one of the most well-maintained and popular open-source libraries available for deep learning. The TensorFlow framework is available in C++ and Python. Other similar deep learning frameworks that are based on Python include Theano, Torch, Lasagne, Blocks, MXNet, PyTorch, and Caffe. You can use TensorBoard for easy visualization and see the computation pipeline. Its flexible architecture allows you to deploy easily on different kinds of devices
On the negative side, TensorFlow does not have symbolic loops and does not support distributed learning. Further, it does not support Windows.2)Theano
Theano is a Python library designed for deep learning. Using the tool, you can define and evaluate mathematical expressions including multi-dimensional arrays. Optimized for GPU, the tool comes with features including integration with NumPy, dynamic C code generation, and symbolic differentiation. However, to get a high level of abstraction, the tool will have to be used with other libraries such as Keras, Lasagne, and Blocks. The tool supports platforms such as Linux, Mac OS X, and Windows.3) Torch
The Torch is an easy to use open-source computing framework for ML algorithms. The tool offers an efficient GPU support, N-dimensional array, numeric optimization routines, linear algebra routines, and routines for indexing, slicing, and transposing. Based on a scripting language called Lua, the tool comes with an ample number of pre-trained models. This flexible and efficient ML research tool supports major platforms such as Linux, Android, Mac OS X, iOS, and Windows.4) Caffe
Caffe is a popular deep learning tool designed for building apps. Created by  Yangqing Jia for a project during his Ph.D. at UC Berkeley, the tool has a good Matlab/C++/ Python interface. The tool allows you to quickly apply neural networks to the problem using text, without writing code. Caffe partially supports multi-GPU training. The tool supports operating systems such as Ubuntu, Mac OS X, and Windows.5) Microsoft CNTK
Microsoft cognitive toolkit is one of the fastest deep learning frameworks with C#/C++/Python interface support. The open-source framework comes with powerful C++ API and is faster and more accurate than TensorFlow. The tool also supports distributed learning with built-in data readers. It supports algorithms such as Feed Forward, CNN, RNN, LSTM, and Sequence-to-Sequence. The tool supports Windows and Linux.6) Keras
Written in Python, Keras is an open-source library designed to make the creation of new DL models easy. This high-level neural network API can be run on top of deep learning frameworks like TensorFlow, Microsoft CNTK, etc.  Known for its user-friendliness and modularity, the tool is ideal for fast prototyping. The tool is optimized for both CPU and GPU.7) SciKit-Learn
SciKit-Learn is an open-source Python library designed for machine learning. The tool based on libraries such as NumPy, SciPy, and matplotlib can be used for data mining and data analysis. SciKit-Learn is equipped with a variety of ML models including linear and logistic regressors, SVM classifiers, and random forests. The tool can be used for multiple ML tasks such as classification, regression, and clustering. The tool supports operating systems like Windows and Linux. On the downside, it is not very efficient with GPU.8)Accord.NET
Written in C#, Accord.NET is an ML framework designed for building production-grade computer vision, computer audition, signal processing and statistics applications. It is a well-documented ML framework that makes audio and image processing easy. The tool can be used for numerical optimization, artificial neural networks, and visualization. It supports Windows.9)Spark MLIib
Apache Spark’s MLIib is an ML library that can be used in Java, Scala, Python, and R. Designed for processing large-scale data, this powerful library comes with many algorithms and utilities such as classification, regression, and clustering. The tool interoperates with NumPy in Python and R libraries. It can be easily plugged into Hadoop workflows.10) Azure ML Studio
Azure ML studio is a modern cloud platform for data scientists. It can be used to develop ML models in the cloud. With a wide range of modeling options and algorithms, Azure is ideal for building larger ML models. The service provides 10GB of storage space per account. It can be used with R and Python programs.11) Amazon Machine Learning
Amazon Machine Learning (AML) is an ML service that provides tools and wizards for creating ML models. With visual aids and easy-to-use analytics, AML aims to make ML more accessible to developers. AML can be connected to data stored in Amazon S3, Redshift, or RDS.Machine learning frameworks come with pre-built components that are easy to understand and code. A good ML framework thus reduces the complexity of defining ML models. With these open-source ML frameworks, you build your ML models easily and quickly.Know an ML framework that should be on this list? Share them in comments below.",,,"https://www.hackerearth.com/blog/developers/,https://www.hackerearth.com/blog/machine-learning/,https://www.tensorflow.org/,http://deeplearning.net/software/theano/,http://torch.ch/,http://caffe.berkeleyvision.org/,http://www.deeplearning.net/software/theano/,http://torch.ch/,http://caffe.berkeleyvision.org/,https://www.microsoft.com/en-us/cognitive-toolkit/,https://keras.io/,https://www.hackerearth.com/challenges/competitive/on-the-plague-trail-hackerearth-machine-learning-challenge/?utm_source=blog&utm_medium=strip,http://scikit-learn.org/,http://accord-framework.net/,https://spark.apache.org/mllib/,https://studio.azureml.net/,https://aws.amazon.com/machine-learning/,"
7,"
Leverage machine learning to amplify your social impact",https://www.hackerearth.com/blog/developers/machine-learning-amplify-social-impact/,"“Data is abundant and cheap but knowledge is scarce and expensive.”In the last few years, there has been a data revolution that has transformed the way we source, capture, and interact with data. From fortune 500 firms to start-ups, healthcare to fintech, machine learning and data science have become an integral part of everyday operations of most companies. Of all the sectors, the social good sector has not seen the push the other sectors have. It is not that the machine learning and data science techniques don’t work for this sector, but the lack of financial support and staff has stopped them from creating their special brand of magic here. At HackerEarth, we intend to tackle this issue by sponsoring machine learning and data science challenges for social good. Even though machine learning (ML) is a new wing at HackerEarth, this is the fastest growing unit in the company. Also, over the past year, we have grown to a community of 200K+ machine learning and data science enthusiasts. We have conducted 50+ challenges across sectors with an average of 6500+ people participating in each.The “Machine Learning Challenges for Social Good” initiative is focused toward bringing interesting real-world data problems faced by nonprofits and governmental and non-governmental organizations to the machine learning and data science community’s notice. This is a win-win for both communities because the nonprofits and governmental and non-governmental organizations get their challenges addressed, and the machine learning and data science community gets to hone their skills while being agents of change. HackerEarth will contribute by Are you a nonprofit or a governmental/non-governmental organization with a business/social problem for which primary or secondary data is available? If yes, please mail us at social@hackerearth.com. [Please use subject line “Reg: Machine Learning for Social Good | {Your Organization Name}]",,"Machine learning at HackerEarth,The initiative,Our role,","https://www.hackerearth.com/blog/community/,https://www.hackerearth.com/blog/developers/,https://www.hackerearth.com/blog/machine-learning/,https://www.hackerearth.com/challenges/competitive/on-the-plague-trail-hackerearth-machine-learning-challenge/?utm_source=blog&utm_medium=strip,mailto:social@hackerearth.com,"
8,"
Artificial  Intelligence 101: How to get started",https://www.hackerearth.com/blog/developers/artificial-intelligence-101-how-to-get-started/,"What is Artificial Intelligence (AI)?Are you thinking of Chappie, Terminator, and Lucy? Sentient, self-aware robots are closer to becoming a reality than you think. Developing computer systems that equal or exceed human intelligence is the crux of artificial intelligence. Artificial Intelligence (AI) is the study of computer science focusing on developing software or machines that exhibit human intelligence. A simple enough definition, right?Obviously, there is a lot more to it. AI is a broad topic ranging from simple calculators to self-steering technology to something that might radically change the future.Goals and Applications of AIThe primary goals of AI include deduction and reasoning, knowledge representation, planning, natural language processing (NLP), learning, perception, and the ability to manipulate and move objects. Long-term goals of AI research include achieving Creativity, Social Intelligence, and General (human level) Intelligence.AI has heavily influenced different sectors that we may not recognize. Ray Kurzweil says “Many thousands of AI applications are deeply embedded in the infrastructure of every industry.” John McCarthy, one of the founders of AI, once said that “as soon as it works, no one calls it AI anymore.”Broadly, AI is classified into the following:Source: BluenotesTypes of AIWhile there are various forms of AI as it’s a broad concept, we can divide it into the following three categories based on AI’s capabilities:Weak AI, which is also referred to as Narrow AI, focuses on one task. There is no self-awareness or genuine intelligence in case of a weak AI.iOS Siri is a good example of a weak AI combining several weak AI techniques to function. It can do a lot of things for the user, and you’ll see how “narrow” it exactly is when you try having conversations with the virtual assistant.Strong AI, which is also referred to as True AI, is a computer that is as smart as the human brain. This sort of AI will be able to perform all tasks that a human could do. There is a lot of research going on in this field, but we still have much to do. You should be imagining Matrix or I, Robot here.Artificial Superintelligence is going to blow your mind if Strong AI impressed you.  Nick Bostrom, leading AI thinker, defines it as “an intellect that is much smarter than the best human brains in practically every field, including scientific creativity, general wisdom and social skills.”Artificial Superintelligence is the reason why many prominent scientists and technologists, including Stephen Hawking and Elon Musk, have raised concerns about the possibility of human extinction.How can you get started?The first thing you need to do is learn a programming language. Though there are a lot of languages that you can start with, Python is what many prefer to start with because its libraries are better suited to Machine Learning.Here are some good resources for Python:Introduction to BotsA BOT is the most basic example of a weak AI that can do automated tasks on your behalf. Chatbots were one of the first automated programs to be called “bots.” You need AI and ML for your chatbots. Web crawlers used by Search Engines like Google are a perfect example of a sophisticated and advanced BOT.You should learn the following before you start programming bots to make your life easier.How can you build your first bot?You can start learning how to create bots in Python through the following tutorial in the simplest way.You can also start by using APIs and tools that offer the ability to build end-user applications. This helps you by actually building something without worrying too much about the theory at first. Some of the APIs that you can use for this are:Here’s a listing of a few BOT problems for you to practice and try out before you attempt the ultimate challenge.What now?Once you have a thorough understanding of your preferred programming language and enough practice with the basics, you should start to learn more about Machine Learning. In Python, start learning Scikit-learn, NLTK, SciPy, PyBrain, and Numpy libraries which will be useful while writing Machine Learning algorithms.You need to know Advanced Math and as well.Here is a list of resources for you to learn and practice:Here are a few more valuable links:You should also take part in various AI and BOT Programming Contests at different places on the Internet:Before you start learning and contributing to the field of AI, read how AI is rapidly changing the world.",,,"https://www.hackerearth.com/blog/artificial-intelligence/,https://www.hackerearth.com/blog/developers/,https://www.hackerearth.com/blog/machine-learning/,https://www.hackerearth.com/blog/python/,https://www.hackerearth.com/challenges/competitive/on-the-plague-trail-hackerearth-machine-learning-challenge/?utm_source=blog&utm_medium=strip,https://dzone.com/articles/top-5-reasons-to-use-artificial-intelligence-in-hu,https://www.hackerearth.com/blog/innovation-management/applications-of-artificial-intelligence/,https://www.codecademy.com/learn/python,http://learnpythonthehardway.org/,https://www.coursera.org/specializations/python,http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-00-introduction-to-computer-science-and-programming-fall-2008/,http://www.512tech.com/technology/bots-101-what-you-need-know-about-chatbots-and-digital-assistants/WFPvQUyYbgo54y2i3RZP7K/,https://www.w3schools.com/xml/xpath_syntax.asp,http://regexr.com/,http://docs.python-requests.org/en/latest/index.html,http://code.tutsplus.com/tutorials/how-to-build-a-python-bot-that-can-play-web-games--active-11117,https://cloud.google.com/prediction/docs,http://www.diffbot.com/products/,http://mallet.cs.umass.edu/,http://scrapy.org/,http://products.wolframalpha.com/api/,https://www.hackerearth.com/problem/multiplayer/tic-tac-toe/,https://www.hackerearth.com/problem/multiplayer/hex/,https://www.hackerearth.com/battle-of-bots-2/multiplayer/dots-and-boxes/,https://www.hackerearth.com/blog/?s=python&submit=Search,http://www.r2d3.us/visual-intro-to-machine-learning-part-1/,https://www.coursera.org/learn/machine-learning,https://www.cs.cmu.edu/~tom/10701_sp11/lectures.shtml,https://www.edx.org/course/artificial-intelligence-ai-columbiax-csmm-101x-4,https://in.udacity.com/course/intro-to-statistics--st101,https://www.udacity.com/course/intro-to-artificial-intelligence--cs271,https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/,http://aima.cs.berkeley.edu/,http://wps.aw.com/wps/media/objects/5771/5909832/PDF/Luger_0136070477_1.pdf,https://grey.colorado.edu/CompCogNeuro/index.php/CCNBook/Main,http://psych.colorado.edu/~oreilly/comp_ex_cog_neuro.html,https://www.hackerearth.com/blog/13-free-self-study-books-mathematics-machine-learning-deep-learning,https://www.hackerearth.com/blog/13-free-training-courses-on-machine-learning-artificial-intelligence,https://datascience.berkeley.edu/blog/data-tools-deep-dive-machine-learning/,https://www.kaggle.com/,http://www.codingame.com/,https://www.hackerearth.com/,http://www.robocup.org/,https://www.hackerearth.com/blog/2016/07/artificial-intelligence-machine-learning-changing-world.html,https://www.hackerearth.com/blog/talent-assessment/tips-to-hire-a-data-scientist/,https://www.hackerearth.com/blog/talent-assessment/proctoring-tips/,https://www.hackerearth.com/blog/talent-assessment/tech-talent-poaching,"
9,Machine Learning Tutorial: What is Machine Learning?,https://www.hackerearth.com/blog/developers/artificial-intelligence-101-how-to-get-started/,"Seems like you would have stumbled upon the term machine learning and must be wondering what exactly it is. Well, this machine learning tutorial will clear out all of your confusion!Machine learning is a field of artificial intelligence with the help of which you can perform magic! Yes, you read it right. Let’s take some real-life examples to understand this. I believe all of you must have heard of Google’s self-driving car. A car that drives by itself without any human support; that is just amazing, isn’t it? Machine Learning Tutorial Machine Learning Tutorial: What is Machine Learning? Seems like you would have stumbled upon the term machine learning and must be wondering what exactly it is. Well, this machine learning tutorial will clear out all of your confusion! Machine learning is a field of artificial intelligence with the help of Now, how about virtual personal assistants such as Apple’s Siri or Microsoft’s Cortana? If you ask Siri what is the distance between Earth and Moon, it will immediately reply that the distance is 384,400km.You also must have used Google maps. If you want to go from New Jersey to New York via road, google maps will show you the distance between these two places, the shortest route and also how much traffic is there along the road.Now, you would agree with me that all of these are some magical applications, and the magic behind these applications is machine learning. So, simply put, machine learning is a sub-domain of artificial intelligence, where a machine is provided data to learn and make insightful decisions.Now, that we have understood what is machine learning, let’s go ahead in this machine learning tutorial and look at the types of machine learning algorithms:Now, let’s go ahead and understand each of these machine learning algorithms comprehensively.In supervised learning, the machine learns from the labeled data, i.e., we already know the result of the input data. In other words, we have input and output variables, and we only need to map a function between the two. The term “supervised learning” stems from the impression that an algorithm learns from a dataset (training). Here, the input is an independent variable, and the output is a dependent variable. The goal is to generate a mapping function that is accurate enough so that the algorithm can predict the output when we feed new input. This is an iterative process. Each time an algorithm makes a prediction, we need to check its performance. If it is not ideal, we have to keep repeating the process. So, this is an apple, isn’t it? Now, how do you know, it’s an apple? Well, as a kid, you would have come across an apple and you were told that it’s an apple and your brain learned that anything which looks like that is an apple.Now, let’s apply the same analogy to a machine. Let’s say we feed in different images of apples to the machine and all of these images have the label “apple” associated with them.Similarly, we will feed in different images of oranges to the machine and all of these images would have the label “orange” associated with them. So, here we are feeding in input data to the machine which is labeled.So, this part in supervised learning, where the machine learns all the features of the input data along with it’s labels is known as ‘training’.Once, the training is done, it will be fed new data or test data to determine, how well the training has been done.So, here, if we feed in this new image of orange to the machine without its label, the machine should be able to predict the correct label based on all of its training.This is the concept of supervised learning, where we train the machine using labeled data and then use this training to find new insights.Now, supervised learning can again be divided into two categories:Moving on in this machine learning tutorial, we will understand these two comprehensively.Since Regression is a supervised learning algorithm, there will be an input variable as well as an output variable and the point to keep in mind is that the output variable is a continuous numerical, i.e. the dependent variable is a continuous numerical.Let’s take this example to understand regression:Let’s say you have two variables, “Number of hours studied” & “Number of marks scored”. Here we want to understand how does the number of marks scored by a student change with the number of hours studied by the student, i.e. “Marks scored” is the dependent variable and “Hours studied” is the independent variable.Based on this data, I now want to know: “How many hours should a student learn to get 60 points?”  So, this is where regression techniques come in. The regression model would understand that there is an increment of 10 marks for every extra hour studied and to score 60 marks the student has to study for 6 hours.You need to note that “marks scored” is the dependent variable and it is a continuous numerical.So, this is how regression algorithms work. Now, let’s move onto the next type of supervised learning algorithms which are classification algorithms.Classification algorithms also need both the input data as well as the output data. Here, the output variable or the dependent variable should be categorical in nature.Let’s take this example to understand classification.Consider these three variables, “Person has lung cancer or not”, “Weight of the person”, “Number of cigarettes smoked in a day”. Here, we want to understand does the person have lung cancer based on the weight of the person and the number of cigarettes he/she smokes in a day, i.e. “Having lung cancer” is the dependent variable and “weight” and “No of cigarettes smoked” are the independent variables. Again, you need to note here that “Having lung cancer” is a categorical variable, which has two categories, “yes” and “No”. Based on the independent variables, we classify whether the person has lung cancer or not.Now, there are a variety of classification algorithms available such as:Let’s go ahead and understand one of these algorithms -> “Decision Tree”.Decision tree is a popular machine learning classifier. So, a decision tree as the name states has an inverted tree like structure. The topmost node in the tree is known as the root node and the nodes at the bottom of the tree are known as the leaf nodes. Every node has a test condition and based on that test condition, the tree splits into either its left child or right child.Let’s go through this example on a decision tree. Here, we are trying to determine whether a person would watch the movie “Avengers” based on a series of test conditions.In unsupervised Learning the machine learns from unlabeled data, i.e. the result for the input data is not known beforehand. Here, the algorithm tries to determine the underlying structure of the data.Now, let’s go through this example to see how does unsupervised learning works.Here, we have a bunch of fruits and none of these fruits have labels associated with them. Now, let’s take these fruits and feed them to an unsupervised learning model. So, the model determines the features associated with the data and understands that all the apples are similar in nature and thus groups them together. Similarly, it understands that all the bananas have the same features and thus group them together and the same is the case with all the mangoes.These are some unsupervised learning algorithms:Further in this machine learning tutorial, we go through the next type of machine learning algorithm – Semi-supervised learning.In semi-supervised learning, the machine learns from a combination of labeled and unlabeled data. In other words, you can consider semi-supervised learning as a fusion of supervised learning and unsupervised learning.Let’s go through this example. Here, we have a bunch of different items -> phones, apples, books, and chairs. Now, as you see over here, only a minor proportion of the items are labeled and the rest are unlabeled. Here, the basic idea is to start off by grouping similar data together. So, all the phones would be put into one group, apples into another and the same is the case with books and chairs.Now we have four clusters containing similar data in them. Here, the algorithm assumes that all the data points which are in proximity tend to have the same label associated with them. Now, the semi-supervised algorithm uses the existing labeled data to assign labels to the rest of the unlabeled data.So, this is the underlying concept of semi-supervised learning. Now, in this machine learning tutorial, let’s head onto the final type of machine learning algorithm, which is reinforcement learning. In reinforcement learning, the algorithm learns through a system of rewards and punishment and the goal here is to maximize the total reward. So, let’s go through this example to understand reinforcement learning.So, here we have a self-driving car that will reach its destination without encountering a roadblock. Thus, the self-driving car is the agent and the road is the environment.The car realizes that going straight is wrong and it has to go right. Evidently, when it goes right, it gets a reward.  Therefore, this process continues and the car learns how to drive by itself without hitting any barricades. And this brings us to the end of this “Machine Learning Tutorial”. We comprehensively understood what is machine learning and then we looked at the types of machine learning. Machine Learning Tutorial Machine Learning Tutorial: What is Machine Learning? Seems like you would have stumbled upon the term machine learning and must be wondering what exactly it is. Well, this machine learning tutorial will clear out all of your confusion! Machine learning is a field of artificial intelligence with the help of Now, if you are interested in doing an end-to-end certification course in Machine Learning, you can check out Intellipaat’s Machine Learning Course with Python.",,"Machine Learning Tutorial: What is Machine Learning?,Watch this Machine Learning Tutorial Video,Watch Top 10 Machine Learning Applications,Machine Learning Tutorial: Supervised Learning,Regression,Classification,Decision Tree Classifier,Watch How to Learn Machine Learning,Machine Learning Tutorial: Unsupervised Learning,Machine Learning Tutorial: Semi-Supervised Learning,Machine Learning Tutorial: Reinforcement Learning,Watch how to start your career in Machine Learning,","https://intellipaat.com/blog/what-is-artificial-intelligence/,https://intellipaat.com/machine-learning-certification-training-course/?utm_source=Machine-Learning-Tutorial&utm_medium=Blog-Tutorial&utm_campaign=may-%2Fmachine-learning-certification-training-course%2F,https://intellipaat.com/machine-learning-certification-training-course/,"
10,Artificial Intelligence :,https://www.hackerearth.com/blog/developers/artificial-intelligence-101-how-to-get-started/,"In the today’s world, there is lot of buzz about artificial intelligence. Do you want to know what exactly it is? So let’s begin!Have you ever thought from where we get the recommendation on Facebook or how does on our way google maps recommends us a faster and alternative route and save our time from heavy traffic conditions? And what about when we are out from home and ask Alexa to control Lighting, AC, Blinds, and Fans.Get certified from top Machine Learning Course Now!Get started with Machine Learning with this short insightful video Introduction Artificial Intelligence : In the today’s world, there is lot of buzz about artificial intelligence. Do you want to know what exactly it is? So let’s begin! Have you ever thought from where we get the recommendation on Facebook or how does on our way google maps recommends us aAI creates a higher degree of efficiency and productivity by automation techniques on the repetitive tasks and it creates an immersive and responsive experience to under human sentiments and even emotions. Artificial Intelligence, the backbone of the next generation software solutions and it is not programmed with all the possible answers but uses what it has learned to create new situations and answers. It is not about building a robot but it is creating a computer which can think like human.Here we don’t have to constantly inputs traffic condition and manually search for traffic conditions when you approach to work. Or we don’t have to teach Alexa how to understand our speech pattern. And we don’t have to map every single road rules and driving scenario for self-driving car to know how to drive.Artificial Intelligence is design to learn this type of functions by its own. We don’t need to continually input information to order to make them function we want.Here we teach machine the mathematical algorithm, the code that helps the computer to learn patterns in data. Now, as a society we are in era of data because of the technology which we are using like our computer, tablets and mobile phone. All these devices are equipped with sensors that produce information that companies can use in their Artificial Intelligence. It helps Artificial Intelligence to learn and make decisions by its own like we do.So are you now interested in mastering Artificial Intelligence, unravel the power of TensorFlow, want to build a real world chatbot that delivers the results?Let’s dive into machine learning. Go through this Artificial Intelligence Interview Questions And Answers to excel in your Artificial Intelligence Interview.Machine learning is a subset of AI that enables the ability of machine to perform at ease, where it can learn and develop from the past without being constantly trained. It is mainly used to develop computer programs that gets data by itself and use it for learning purpose. There are two kinds of learning in machine learning –Watch Machine Learning Full Course Tutorial Introduction Artificial Intelligence : In the today’s world, there is lot of buzz about artificial intelligence. Do you want to know what exactly it is? So let’s begin! Have you ever thought from where we get the recommendation on Facebook or how does on our way google maps recommends us a If you have any doubts or queries related to Data Science, do post on Machine Learning Community.",,"Artificial Intelligence :,So, how does Artificial Intelligence actually do this?,Machine learning Basics :,","https://intellipaat.com/blog/what-is-artificial-intelligence/,https://intellipaat.com/machine-learning-certification-training-course/,https://intellipaat.com/machine-learning-certification-training-course/?utm_source=Machine-Learning-Tutorial&utm_medium=Blog-Tutorial&utm_campaign=may-%2Fmachine-learning-certification-training-course%2F,https://intellipaat.com/blog/tutorial/machine-learning-tutorial/tensorflow-andits-installation-windows/,https://intellipaat.com/blog/how-to-build-an-artificial-intelligence-chatbot/,https://intellipaat.com/blog/interview-question/artificial-intelligence-interview-questions/,https://intellipaat.com/community/machine-learning,"
11,What Are the Different Types of Machine Learning Algorithms?,https://www.hackerearth.com/blog/developers/artificial-intelligence-101-how-to-get-started/,"There are different ways of how a machine learns. In some cases, we train them and, in some other cases, machines learn by their own. Well, primarily, there are two types of machine learning – Supervised Learning and Unsupervised Learning. In this module, we are going to discuss the types of machine learning in detail.Learn Machine Learning from experts, click here to more in this Machine Learning Training in Hyderabad! In case you want to jump right into a particular topic, here’s the table of contents for this module:Without further ado, let’s get started.The type of learning algorithm where the input and the desired output are provided is known as the Supervised Learning Algorithm. In Supervised Machine Learning, labeled data is used to train machines in order to make them learn and establish relationships between given inputs and outputs. Now, you must be wondering what labeled data means, right? Well, a label is nothing but a known description or a tag given to objects in the data. For instance, you have a dataset that consists of information related to 10 different patients with respective symptoms and their cancer test results. Based on the test results, you can put a tag on each patient specifying whether they are cancer positive or cancer negative.So, labels in a given data also provide the structure of the algorithm output, that is, any result must be one of these labels. Now comes another question, how does labeled data help in this algorithm? Machines, to learn the patterns, classify this data and apply these patterns to classify new data. You can also put it in this way, when you have labeled input data and you know what needs to be predicted, then you can use Supervised Machine Learning.Now that we are familiar with what Supervised Machine Learning Algorithm means, let us explore how it works.  Step 1: The very first step of Supervised Machine Learning is to load labeled data into the system. This step is a bit time consuming, because the preparation of labeled data is often done by a human trainer. Here, the dataset is divided into train and test sets for further operations. Step 2: The next step is to train and build connections of inputs and outputs. This step is also known as the training model. Step 3: Then comes the step known as the testing model. As the name suggests, you test the model by introducing it to a set of new data. Let us understand this with the help of an example. Suppose, we have a labeled dataset that consists of images of cats and dogs, with different attributes such as nose, tongue, ear etc. Now, we are going to divide this labeled dataset into train and test sets.  The image of a dog shown above has labels such as ears, nose, tongue, and dog. We train the model with this image. Then, we repeat the same training process with other images of both cats and dogs with their attributes. Once the model is trained and the algorithm is built, the accuracy can be tested with the help of a test dataset. When we feed the model with a new dog image, it scans the image and matches the attributes of the image with other trained images. Then depending upon the accuracy of the model, it returns the output ‘dog.’ Now, remember that it takes a large amount of data to build a model with a good accuracy percentage. That is one drawback of this type of algorithms. Let us discuss some positive and negative sides of the Supervised Machine Learning Algorithm. Become Master of Machine Learning by going through this online Machine Learning course in Sydney.As you might have noticed, in Supervised Machine Learning, the objective is very clear. For example, we want to predict whether the animal in a particular image is a dog or a cat. The training of the machine is tightly controlled, which in return gives an outcome of a very specific behavior. Also, the accuracy of these models can be measured easily.On the other hand, it often becomes labor-intensive as the data requires labeling before the model is trained, which can take hours of human effort. The cost becomes astronomical then and the training process gets slowed down. After preprocessing the data, we might have to eliminate useless data. This might limit the data that the system can work with. Another drawback of this algorithm is that we limit the insight for a machine to explore, as the predicted behavior is mentioned in advance. There is no freedom for the machine to explore other possibilities, unlike in Unsupervised Machine Learning.Supervised Machine learning has primarily two types of Machine Learning algorithms. They are classification in machine learning and regression in machine learning.Classification in machine learning algorithm classifies the input data into one of several predefined classes. Classification in machine algorithm is useful for providing categorical outcomes that fit within the predefined labels.Credit card fraud detection and email spam detection are the use cases of binary classification. There are only two possible output values in this type of algorithms. They can be either fraud or not fraud and spam or non-spam.  But if the question we are asking does not have potential categorical answers, then we aren’t dealing with a problem of classification in machine learning, it is more of a problem that falls under regression in machine learning. Let us discuss what regression in machine learning is.It is a predictive algorithm that attempts to predict the output value when the input value is given. It deals with continuous numerical values. It estimates the relationship between variables.This type of algorithm can be used to determine if a customer is going to churn or not, depending upon the customers behavior. It can also be used to predict housing price models.  Regression in Machine Learning defines the strength of correlation between two attributes, which allows us to find a predictive range of likelihood.Go through this Artificial Intelligence Interview Questions And Answers to excel in your Artificial Intelligence Interview.It is very important to understand which Machine Learning algorithm should be applied depending upon the type of problem we are dealing with. The below table consists of different types of Machine Learning problems and the possible algorithms for solving those problems.  As we have seen that Supervised Machine Learning deals with known labeled data for prediction, you must be thinking what if we don’t have labeled data? How to apply learning algorithms for unlabeled data? Well, for that we have Unsupervised Machine Learning. Let us explore the next types of machine learning, that is, Unsupervised Learning. Unsupervised Machine Machine Learning type of Machine Learning models, we don’t have labeled data. Since we are not aware of the predefined outcome, there are certain questions that are left hanging for us to wonder: How to find the underlying structure of a given dataset? How to summarize it or group it usefully? In a way, these can be considered as primary goals of this type of Machine Learning. Since there is no specific outcome or target to predict, this Machine Learning type is called ‘Unsupervised Machine Learning.’ When we don’t know how to classify the given data but we want the machine to group or classify it for us, use this Machine Learning technique. Now, let us try to understand how Unsupervised Machine Learning works.When the data given is not labeled, the following steps are followed in order to learn and gain insights:  Step 1: The very first step is to load the unlabeled data into the system. Step 2: Once the data is loaded into the system, the algorithm analyzes the data. Step 3: As the analysis gets completed, the algorithm will look for patterns depending upon the behavior or attributes of the dataset. Step 4: Once pattern identification and grouping are done, it gives the output.Not having labeled data turns out to be good in some cases. Unsupervised Machine Learning techniques are much faster to implement compared to Supervised Machine Learning, since no data labeling is required here. That is, fewer human resources is required in order to perform tasks. This algorithm has the potential to provide unique, disruptive insights for a business to consider as it interprets data on its own. But on the downside, in Unsupervised Machine Learning, it is not easy to measure the accuracy since we don’t have any expected or desired outcome to compare to. Sometimes, it requires more tuning in order to get meaningful results. Also, it does not naturally deal with high-dimensional data. When the dimension of data and the number of variables become more and need to be reduced in order to work on that data, then the human involvement becomes necessary to clean the data. Primarily, there two categories in Unsupervised Machine Learning: Clustering and Dimensionality Reduction. If you have any doubts or queries related to Data Science, do post on Machine Learning Community.Various types of Machine Learning algorithms include clustering algorithm, which runs through the given data to find natural clusters if they exist. There are a few different clustering techniques but remember that any clustering algorithm will typically output all of the data points in their respective clusters. Now, it is totally up to us to decide what they actually mean and exactly what the algorithm has found. There are different types of machine learning clustering techniques available. They are:Interested in learning Machine Learning? Click here to learn more in this Machine Learning Training in New York!This module highlighted the primary machine learning types, their workings, subcategories, regression in machine learning, classification in machine learning, clustering in machine learning, dimensionality reduction in machine learning, their use cases, and the pros and cons of different types of Machine learning. In the next module, we will be discussing different aspects related to datasets for Machine Learning. See you there.",,"What Are the Different Types of Machine Learning Algorithms?,Watch this Supervised vs Unsupervised vs Reinforcement Learning Tutorial,Types of Supervised Machine Learning Algorithm,Unsupervised Machine Learning,How Does Unsupervised Machine Learning Works?,Watch this Introduction to Reinforcement Learning Tutorial,What Did We Learn so Far?,","https://intellipaat.com/machine-learning-certification-training-course-hyderabad/,#supervised-learning,#how-learning,#pros-cons-learning,#types-supervised-learning,#classification,#Regression,#supervised-algorithm-table,#unsupervised-learning,#how-unsupervised-learning,#pros-cons-learning,#types-unsupervised-learning,#Clustering,#Dimensionality-Reduction,https://intellipaat.com/blog/what-is-machine-learning/,https://intellipaat.com/mediaFiles/2019/04/ml.png,https://intellipaat.com/machine-learning-certification-training-course/?utm_source=Machine-Learning-Tutorial&utm_medium=Blog-Tutorial&utm_campaign=may-%2Fmachine-learning-certification-training-course%2F,https://intellipaat.com/machine-learning-course-in-sydney/,https://intellipaat.com/machine-learning-certification-training-course/,https://intellipaat.com/blog/interview-question/artificial-intelligence-interview-questions/,https://intellipaat.com/artificial-intelligence-masters-training-course/?utm_source=Machine-Learning-Tutorial&utm_medium=Blog-Tutorial&utm_campaign=may-%2Fartificial-intelligence-masters-training-course%2F,https://intellipaat.com/community/machine-learning,https://intellipaat.com/all-courses/big-data/?utm_source=salesforce-tutorial-page&utm_medium=Tutorial-Page&utm_campaign=May-all-courses%2Fbig-data%2F,https://intellipaat.com/machine-learning-course-in-new-york/,"
12,How Do We Get the Right Dataset for Machine Learning?,https://www.hackerearth.com/blog/developers/artificial-intelligence-101-how-to-get-started/,"Data is the most important component of Machine Learning. In order to train models, we should have the ‘right data’ in the ‘right format.’ Now, you must be thinking how do we get the right data, right? Well, getting the right data means collecting or identifying the data that correlates with the outcomes which need to be predicted. In other words, data needs to be aligned with the problem we are trying to solve. Also, the data used to build the model should not be non-representative, error-ridden, and of low quality. So let’s see how to get the right datasets for Machine learning  In this module, we will be discussing the following topics:Without more delay, let’s get started.Data collection is considered as the foundation of the Machine Learning model building. Without data, the concept of building a Machine Learning model is futile. The more data we have the better predictive model we can build out of it. But remember, ‘more data’ does not mean a bunch of irrelevant data. We cannot add any data just to increase the quantity. So, we can say that any effort that is directed toward ‘finding the right data’ is well invested—that way after putting the collected data through a cleansing process, we will have ‘more data’ to build the model with. Now, I am sure that you must be wondering how we can find dataset for machine learning operations. Dataset for machine learning can be found in two formats—structured and unstructured. Let us elaborate on what structured and unstructured dataset for machine learning are. Structured data is highly organized. It is comprised of clearly defined data types which are easy to digest. More importantly, structured data is easily searchable. Whereas, unstructured data, with no defined data types, is not easily searchable. The below image provides further differences between structured and unstructured data.  Structured data can be displayed in rows and columns and, usually, it resides in relational databases (RDMS). Data can be created by human or machine, as long as it is fit to reside in an RDMS, it can be searchable both by human-generated queries and by using algorithms using type of data and field names. Typical structured data includes dates, phone numbers, credit card numbers, customer names, addresses, product names and numbers, transaction details, etc. Unstructured data can be textual or non-textual, human or machine generated; it may also be in non-relational databases like NoSQL. It does not fit in relational databases. Human-generated unstructured data includes email text files, social media data, location-based data, and media files such as MP3, digital photo, audio, and video files. Typical machine-generated data includes weather data, surveillance photos and videos, sensor-based traffic data, etc. Structured data requires less storage space, which makes it easier to manage. But unstructured data requires more storage space. According to Gartner, unstructured data makes up to 80 percent of the enterprise data. Unstructured data is growing in an insane manner. According to IDC, unstructured data grows at 26.8 percent annually compared to the structured data, which grows at 19.6 percent annually. Due to the sheer volume of the unstructured data, traditional data collecting techniques often leave out valuable information. That is why the unstructured data management needs to be different. Today’s enterprises need a separate data management platform that’s built specifically to handle unstructured data. There is a plethora of open-source datasets available for us to exercise Machine Learning Algorithms on. Here, we are listing out some of those.This blog briefs about gathering different datasets for machine learning and describes how data exists in different forms, namely, structured and unstructured. Also, we listed out some of the sources, like uci machine learning datasets, to exercise our Machine Learning models on. In the next module, we will be discussing about different aspects related to datasets for Machine Learning. See you there.",,"How Do We Get the Right Dataset for Machine Learning?,Gathering Datasets for Machine Learning,List of Open-source Datasets for Machine Learning,What Did We Learn so Far?,","#_Gathering_Data,#_Structured_V_Unstructured,#_list_of_open,https://intellipaat.com/machine-learning-certification-training-course/?utm_source=Machine-Learning-Tutorial&utm_medium=Blog-Tutorial&utm_campaign=may-%2Fmachine-learning-certification-training-course%2F,https://intellipaat.com/artificial-intelligence-masters-training-course/?utm_source=Machine-Learning-Tutorial&utm_medium=Blog-Tutorial&utm_campaign=may-%2Fartificial-intelligence-masters-training-course%2F,https://intellipaat.com/all-courses/big-data/?utm_source=salesforce-tutorial-page&utm_medium=Tutorial-Page&utm_campaign=May-all-courses%2Fbig-data%2F,"
13, Preprocessing Data,https://www.hackerearth.com/blog/developers/artificial-intelligence-101-how-to-get-started/,"Data preprocessing is a way of converting data from the raw form to a much more usable or desired form, i.e., making data more meaningful by rescaling, standardizing, binarizing, one hot encoding, and label encoding.  The process of getting raw data ready for a Machine Learning algorithm can be summarized in the below steps:  Here’s the list of contents for this module. Alright, let’s get started.For the best of career growth, check out Intellipaat’s Machine Learning Course and get certified. As the name suggests, rescaling data is the process of making non-uniform attributes of a dataset uniform. Now, the question is when we would know that a dataset is uniform or not. Well, when the scale of attribute varies widely that can be rather harmful to our predictive model, we call it a non-uniform dataset. Rescaling method is useful in optimization algorithms such as in gradient descent. It is done using MinMaxScaler class which comes under scikit-learn, also known as sklearn.  Now, let us explore this method with an example. First, we will take a look at the dataset that we are going to perform rescaling on.  Dataset: The ‘winequality-red.csv’ dataset is used for explaining the data preprocessing methods here. This csv dataset looks something like this:Alright, let us perform rescaling now.Output: Here, we have rescaled the values from a wide scale into a range that lies between 0 and 1. Alright, the next method of data preprocessing is standardizing. Become Master of Machine Learning by going through this online Machine Learning course in Sydney.Standardizing data helps us transform attributes with a Gaussian distribution of differing means and of differing standard deviations into a standard Gaussian distribution with a mean of 0 and a standard deviation of 1. Standardization of data is done using scikit-learn with the StandardScaler class.In this method, all the values that are above the threshold are transformed into 1 and those equal to or below the threshold are transformed into 0. This method is useful when we deal with probabilities and need to convert the data into crisp values. Binarizing is done using the Binarizer class.Output:Go through this Artificial Intelligence Interview Questions And Answers to excel in your Artificial Intelligence Interview.While dealing with categorical data, one hot encoding is performed using the OneHotEncoder class.Output:Output:Interested in learning Machine Learning? Click here to learn more in this Machine Learning Training in New York!Labels can be words or numbers. Usually, the training data is labeled with words to make it readable. Label encoding converts word labels into numbers to let algorithms work on them.Output:Output:Output:Output:If you have any doubts or queries related to Data Science, do post on Machine Learning Community.In this module, we have discussed on various data preprocessing methods for Machine Learning such as rescaling, binarizing, standardizing, one hot encoding, and label encoding. In the next module, we will be diving into training, validation, and testing datasets. Let’s meet there!",," Preprocessing Data,Rescaling Data,Standardizing Data,Binarizing Data,One Hot Encoding,Label Encoding,What Did We Learn?,","https://intellipaat.com/blog/tutorial/machine-learning-tutorial/machine-learning-algorithms/,https://intellipaat.com/machine-learning-certification-training-course/,https://intellipaat.com/machine-learning-course-in-sydney/,https://intellipaat.com/blog/interview-question/artificial-intelligence-interview-questions/,https://intellipaat.com/machine-learning-course-in-new-york/,https://intellipaat.com/community/machine-learning,"
14,Understanding Machine Learning,https://www.hackerearth.com/blog/developers/artificial-intelligence-101-how-to-get-started/,"The term ‘Machine Learning’ seems to be a hot cake these days. So, what exactly is it? Well, simply put, Machine Learning is the sub-field of Artificial Intelligence, where we teach a machine how to learn, with the help of input data. Now that we know, what exactly is machine learning, let’s have a look at the types of Machine Learning algorithms.Machine Learning Algorithms can be grouped into two types:Learn Machine Learning from experts, click here to more in this Machine Learning Training in London!In supervised machine learning algorithms, we have input variables and output variables. The input variables are denoted by ‘x’ and the output variables are denoted by ‘y’.  Here, the aim of supervised learning is to understand, how does ‘y’ vary with ‘x’, i.e. the goal is to approximate the mapping function so well that when we have a new input data (x) we can predict the output variables (Y) for that data. Or, in other words, we have dependent variables and independent variables and our aim is to understand how does a dependent variable change with respect to an independent variable. Let’s understand supervised learning through this example:  Here, our independent variable is “Gender” of the student and dependent variable is “Output” of the student and we are trying to determine whether the student would pass the exam or not based of the student’s gender. Now, supervised learning can again be divided into regression and classification, so let’s start with regression.In regression, the output variable is a continuous numeric value. So, let’s take this example to understand regression better:  Here, the output variable is the “cost” of apple, which is a continuous value, i.e. we are trying to predict the “cost” of apple with respect to other factors. Now, it’s time to look at one of the most popular regression algorithm -> Linear Regression.For the best of career growth, check out Intellipaat’s Machine Learning Course and get certified. As the name states, linear regression is used to determine the linear relationship between independent and dependent variable. Or in other words, it is used in estimating exactly how much of y will linearly change, when x changes by a certain amount.  As we see in the image, a car’s mpg(Miles per Gallon) is mapped onto the x-axis and the hp(Horse Power) is mapped on the y-axis and we are determining if there is a linear relationship between “hp” and “mpg”. So, this was the linear regression algorithm, now let’s head onto classification in machine learning.In classification, the output variable is categorical in nature. So, let’s take this example to understand classification better:  Here, the output variable is the “gender” of the person, which is a categorical value and we are trying to classify the person into a specific gender based on other factors. Now, we’ll look at these classification algorithms in brief:Decision tree is one of the most used machine learning algorithms in use, currently. As the name suggests, in Decision Tree, we have a tree-like structure of decisions and their possible consequences. At each node there is a test condition and the node splits into left and right children based on the test condition.  Now, let’s look at some terminologies of decision tree:Become Master of Machine Learning by going through this online Machine Learning course in Singapore.As the name states, random forest is an ensemble of multiple decision tree models. In this algorithm, random subsets are generated from the original dataset. Let’s say, if ‘x’ datasets are created from the original dataset, then, ‘x’ decision trees are built on top of these datasets. So, each of these ‘decision trees generate a result and the optimal solution is found out by taking the aggregate of all the individual results.  So, these were some of the classification algorithms, now, let’s head onto unsupervised learning: In unsupervised machine learning algorithms, we have input data with no class labels and we build a model to understand the underlying structure of the data. Let’s understand this with an example:  Here, we have input data with no class labels and this input data comprises of fish and birds. Now, let’s build an unsupervised model on top of this input data. So, this will give out two clusters. The first cluster comprises of all the fish and the second cluster comprises of all the birds.Now, you guys need to keep in mind that even though there were no class labels, this unsupervised learning model was able to divide this data into two clusters and this clustering has been done on the basis of similarity of characteristics. Now, out of all the unsupervised machine learning algorithms, k-means clustering is the most popular, so let’s understand that.Go through this Artificial Intelligence Interview Questions And Answers to excel in your Artificial Intelligence Interview.K means clustering is an unsupervised machine learning algorithm, where the aim is to group similar data points into a single cluster. So, there must be high intra-cluster similarity and low inter-cluster similarity, i.e. all the data points within a cluster should be as similar as possible and the data points between two different clusters should be as dissimilar as possible.  In k-means clustering, ‘k’ denotes the number of clusters to be formed. So, in the above picture, the value of k=3 and hence 3 clusters are formed. Machine Learning Algorithms Understanding Machine Learning The term ‘Machine Learning’ seems to be a hot cake these days. So, what exactly is it? Well, simply put, Machine Learning is the sub-field of Artificial Intelligence, where we teach a machine how to learn, with the help of input data. Now that we know, what So, these were some of the most popular machine learning algorithms.",,"Understanding Machine Learning,Types of Machine Learning Algorithms,Supervised Learning,Regression in Machine Learning,Linear Regression,Watch this Machine Learning Algorithms Tutorial,Classification in Machine Learning,Decision Tree,Watch this Machine Learning Algorithms Tutorial,Random Forest,Unsupervised Learning,K-means Clustering,Watch this K Means Clustering Tutorial,","https://intellipaat.com/blog/what-is-machine-learning/,https://intellipaat.com/blog/what-is-artificial-intelligence/,https://intellipaat.com/blog/tutorial/machine-learning-tutorial/types-of-machine-learning/,https://intellipaat.com/mediaFiles/2019/07/ML1.png,https://intellipaat.com/mediaFiles/2019/07/ML2.png,https://intellipaat.com/machine-learning-course-in-london/,https://intellipaat.com/mediaFiles/2019/07/ML3.png,https://intellipaat.com/mediaFiles/2019/07/ML4.png,https://intellipaat.com/mediaFiles/2019/07/ML5.png,https://intellipaat.com/blog/what-is-linear-regression/,https://intellipaat.com/machine-learning-certification-training-course/,https://intellipaat.com/mediaFiles/2019/07/ML6.png,https://intellipaat.com/blog/tutorial/machine-learning-tutorial/classification-machine-learning/,https://intellipaat.com/mediaFiles/2019/07/ML7.png,https://intellipaat.com/mediaFiles/2019/07/ML8.png,https://intellipaat.com/blog/decision-tree-algorithm-in-machine-learning/,https://intellipaat.com/machine-learning-course-in-singapore/,https://intellipaat.com/machine-learning-certification-training-course/?utm_source=Machine-Learning-Tutorial&utm_medium=Blog-Tutorial&utm_campaign=may-%2Fmachine-learning-certification-training-course%2F,https://intellipaat.com/mediaFiles/2019/07/ML9.png,https://intellipaat.com/mediaFiles/2019/07/ML10.png,https://intellipaat.com/blog/interview-question/artificial-intelligence-interview-questions/,https://intellipaat.com/mediaFiles/2019/07/ML11.png,"
15,Classification in Machine Learning,https://www.hackerearth.com/blog/developers/artificial-intelligence-101-how-to-get-started/,"Supervised learning techniques can be broadly divided into regression and classification algorithms. In this session, we will be focusing on classification in Machine Learning. We’ll go through the below example to understand classification in a better way.  Let’s say, you live in a gated housing society and your society has separate dustbins for different types of waste: one for paper waste, one for plastic waste, and so on. What you are basically doing over here is classifying the waste into different categories. So, classification is the process of assigning a ‘class label’ to a particular item. In the above example, we are assigning the labels ‘paper’, ‘metal’, ‘plastic’, and so on to different types of waste.For the best of career growth, check out Intellipaat’s Machine Learning Course and get certified. Now that we know what exactly classification is, we will be going through the classification algorithms in Machine Learning:Logistic regression is a binary classification algorithm which gives out the probability for something to be true or false. Let’s take this example to understand logistic regression:  Here, we have two independent variables ‘Temperature’ and ‘Humidity’, while the dependent variable is ‘Rain’. We are trying to determine the probability of raining, on the basis of different values for ‘Temperature’ and ‘Humidity’. Logistic regression is an estimation of the logit function and the logit function is simply a log of odds in favor of the event. Go through this Artificial Intelligence Interview Questions And Answers to excel in your Artificial Intelligence Interview.Decision tree, as the name states, is a tree-based classifier in Machine Learning. You can consider it to be an upside-down tree, where each node splits into its children based on a condition. Let’s take this example to understand the concept of decision trees:  Here, we are building a decision tree to find out if a person is fit or not. Based on a series of test conditions, we finally arrive at the leaf nodes and classify the person to be fit or unfit.Random Forest is an ensemble technique, which is basically a collection of multiple decision trees.  Here, we generate multiple subsets of our original dataset and build decision trees on each of these subsets. As we see in the above picture, if we generate ‘x’ subsets, then our random forest algorithm will have results from ‘x’ decision trees. The final solution would be the average vote of all these results. Naive Bayes is a probabilistic classifier in Machine Learning which is built on the principle of Bayes theorem. Naive Bayes classifier makes an assumption that one particular feature in a class is unrelated to any other feature and that is why it is known as naive.  The below picture denotes the Bayes theorem:  So, these are some most commonly used algorithms for classification in Machine Learning.If you have any doubts or queries related to Data Science, do post on Machine Learning Community.",,"Classification in Machine Learning,Classification Algorithms in Machine Learning,Logistic Regression,Watch this Logistic Regression Tutorial,Decision Tree,Watch this Decision Tree Machine Learning Tutorial,Random Forest,Naive Bayes,","https://intellipaat.com/mediaFiles/2019/07/ML10.jpg,https://intellipaat.com/machine-learning-certification-training-course/,https://intellipaat.com/blog/what-is-logistic-regression/,https://intellipaat.com/mediaFiles/2019/07/ML12.png,https://intellipaat.com/mediaFiles/2019/07/ML13.png,https://intellipaat.com/blog/interview-question/artificial-intelligence-interview-questions/,https://intellipaat.com/blog/decision-tree-algorithm-in-machine-learning/,https://intellipaat.com/mediaFiles/2019/07/ML14.png,https://intellipaat.com/blog/what-is-random-forest-algorithm-in-python/,https://intellipaat.com/mediaFiles/2019/07/ML15.png,https://intellipaat.com/machine-learning-certification-training-course/?utm_source=Machine-Learning-Tutorial&utm_medium=Blog-Tutorial&utm_campaign=may-%2Fmachine-learning-certification-training-course%2F,https://intellipaat.com/mediaFiles/2019/07/ML16.png,https://intellipaat.com/mediaFiles/2019/07/ML20.jpg,https://intellipaat.com/community/machine-learning,"
16,What is Support Vector Machine? SVM Algorithm in Machine Learning,https://www.hackerearth.com/blog/developers/artificial-intelligence-101-how-to-get-started/,"Support Vector Machine or SVM algorithm is a simple yet powerful Supervised Machine Learning algorithm that can be used for building both regression and classification models. SVM algorithm can perform really well with both linearly separable and non-linearly separable datasets. Even with a limited amount of data, the support vector machine algorithm does not fail to show its magic. SVM Figure 1: Linearly Separable and Non-linearly Separable DatasetsBefore diving right into understanding the support vector machine algorithm in Machine Learning, let us take a look at the important concepts this blog has to offer.Interested in learning Machine Learning? Click here to learn more in this Machine Learning Training in Bangalore!Support vector machine or SVM algorithm is based on the concept of ‘decision planes’, where hyperplanes are used to classify a set of given objects. Let us start off with a few pictorial examples of support vector machine algorithm. As we can see in Figure 2, we have two sets of data. These datasets can be separated easily with the help of a line, called a decision boundary. SVM Figure 2: Decision BoundaryBut there can be several decision boundaries that can divide the data points without any errors. For example, in Figure 3, all decision boundaries classify the datasets correctly. But how do we pick the best decision boundary? SVM Figure 3: Other Possible Decision BoundariesWell, here’s the tip: the best decision boundary is the one which has maximum distance from the nearest points of these two classes, as shown in Figure 4. SVM Figure 4: Maximum Distance from the Nearest PointsAlso remember that the nearest points from the optimal decision boundary that maximize the distance are called support vectors. SVM Figure 5: Margin and Maximum Margin ClassifierThe region that the closest points define around the decision boundary is known as the margin. That is why the decision boundary of a support vector machine model is known as the maximum margin classifier or the maximum margin hyperplane.  In other words, here’s how a support vector machine algorithm model works:Alright, in the above support vector machine example, the dataset was linearly separable. Now, the question, how do we classify non-linearly separable datasets as shown in Figure 6? SVM Figure 6: Non-linearly Separable DatasetClearly, straight lines can’t be used to classify the above dataset. That is where Kernel SVM comes into the picture. SVM Figure 7: After Using Kernel Support Vector ClassifierWhat does Kernel SVM do? How does it find the classifier? Well, the Kernel SVM projects the non-linearly separable datasets of lower dimensions to linearly separable data of higher dimensions. Kernel SVM performs the same in such a way that datasets belonging to different classes are allocated to different dimensions. Interesting, isn’t it? Well, before exploring how to implement SVM in Python programming language, let us take a look at the pros and cons of support vector machine algorithm.Become Master of Machine Learning by going through this online Machine Learning course in Singapore.SVM libraries are packed with some popular kernels such as Polynomial, Radial Basis Function or rbf, and Sigmoid. The classification function used in SVM in Machine Learning is SVC. The SVC function looks like this: sklearn.svm.SVC (C=1.0, kernel= ‘rbf’, degree=3) Important parameters are:Alright, let us dive right into the hands-on of SVM in Python programming language.If you have any doubts or queries related to Data Science, do post on Machine Learning Community.Problem Statement: Use Machine Learning to predict cases of breast cancer using patient treatment history and health data Dataset: Breast Cancer Wisconsin (Diagnostic) Dataset Let us have a quick look at the dataset:  Classification Model Building: Support Vector Machine in Python Let us build the classification model with the help of a Support Vector Machine algorithm. Step 1: Load Pandas library and the dataset using Pandas  Let us have a look at the shape of the dataset:   Step 2: Define the features and the target  Have a look at the features:  Have a look at the target:   Step 3: Split the dataset into train and test using sklearn before building the SVM algorithm model  Step 4: Import the support vector classifier function or SVC function from Sklearn SVM module. Build the Support Vector Machine model with the help of the SVC function  Step 5: Predict values using the SVM algorithm model  Step 6: Evaluate the Support Vector Machine model Go through this Artificial Intelligence Interview Questions And Answers to excel in your Artificial Intelligence Interview.Importing the libraries: Importing the SVC function and setting kernel as ‘poly’:   Making predictions:  Evaluating the model:  Importing the SVC function and setting kernel as ‘rbf’:   Making predictions:   For the best of career growth, check out Intellipaat’s Machine Learning Course and get certified. Importing the SVC function and setting SVM kernel as ‘sigmoid’:   Making predictions:  Evaluating the model:   In this SVM tutorial blog, we answered the question, ‘what is SVM?’ Some other important concepts such as SVM full form, pros and cons of SVM algorithm, and SVM examples, are also highlighted in this blog . We also learned how to build support vector machine models with the help of the support vector classifier function. Additionally, we talked about the implementation of Kernel SVM in Python and Sklearn, which is a very useful method while dealing with non-linearly separable datasets.Compare SVM Machine Learning model with other Supervised Machine Learning classification models like Random Forest and Decision Tree!",,"What is Support Vector Machine? SVM Algorithm in Machine Learning,Support Vector Machine Algorithm Example,Advantages of Support Vector Machine Algorithm,Disadvantages of Support Vector Machine Algorithm,How Does the Support Vector Machine Algorithm Work?,,Building a Support Vector Machine Classification Model in Machine Learning Using Python ,Implementing Kernel SVM with Sklearn SVM module,What did we learn so far?,","https://intellipaat.com/mediaFiles/2019/07/SVM2.png,#Support-Vector-Machine-Algorithm-Example,#Advantages-Support-Vector-Machine-Algorithm-Example,#Disadvantages-Support-Vector-Machine-Algorithm-Example,#How-Does-Support-Vector-Machine-Work,#Building-a-Support-Vector-Machine-Classification-Model-in-Machine-Learning-Using-Python,#Implementation-of-Kernel-SVM-with-Sklearn-SVM-Module,#Polynomial-SVM-Kernele,#Gaussian-SVM-Kernel,#Sigmoid-SVM-Kernel,https://intellipaat.com/machine-learning-certification-training-course-bangalore/,https://intellipaat.com/mediaFiles/2019/07/SVM3.png,https://intellipaat.com/mediaFiles/2019/07/SVM44.png,https://intellipaat.com/mediaFiles/2019/07/SVM6.png,https://intellipaat.com/mediaFiles/2019/07/SVM7.png,https://intellipaat.com/machine-learning-certification-training-course/?utm_source=Machine-Learning-Tutorial&utm_medium=Blog-Tutorial&utm_campaign=may-%2Fmachine-learning-certification-training-course%2F,https://intellipaat.com/mediaFiles/2019/07/SVM8.png,https://intellipaat.com/mediaFiles/2019/07/SVM9.png,https://intellipaat.com/machine-learning-course-in-singapore/,https://intellipaat.com/artificial-intelligence-masters-training-course/?utm_source=Machine-Learning-Tutorial&utm_medium=Blog-Tutorial&utm_campaign=may-%2Fartificial-intelligence-masters-training-course%2F,https://intellipaat.com/community/machine-learning,https://intellipaat.com/mediaFiles/2019/07/SVM10.png,https://intellipaat.com/mediaFiles/2019/07/SVM11-2.png,https://intellipaat.com/mediaFiles/2019/07/SVM12.png,https://intellipaat.com/mediaFiles/2019/07/SVM13.png,https://intellipaat.com/mediaFiles/2019/07/SVM14.png,https://intellipaat.com/mediaFiles/2019/07/SVM15.png,https://intellipaat.com/mediaFiles/2019/07/SVM16.png,https://intellipaat.com/mediaFiles/2019/07/SVM17.png,https://intellipaat.com/mediaFiles/2019/07/SVM18.png,https://intellipaat.com/mediaFiles/2019/07/SVM19.png,https://intellipaat.com/mediaFiles/2019/07/SVM20.png,https://intellipaat.com/mediaFiles/2019/07/SVM22.png,https://intellipaat.com/mediaFiles/2019/07/SVM23.png,https://intellipaat.com/mediaFiles/2019/07/SVM24.png,https://intellipaat.com/blog/interview-question/artificial-intelligence-interview-questions/,https://intellipaat.com/mediaFiles/2019/07/SVM25.png,https://intellipaat.com/mediaFiles/2019/07/SVM26.png,https://intellipaat.com/mediaFiles/2019/07/SVM27.png,https://intellipaat.com/mediaFiles/2019/07/SVM28.png,https://intellipaat.com/mediaFiles/2019/07/SVM33.png,https://intellipaat.com/mediaFiles/2019/07/SVM30.png,https://intellipaat.com/mediaFiles/2019/07/SVM34.png,https://intellipaat.com/mediaFiles/2019/07/SVM35.png,https://intellipaat.com/mediaFiles/2019/07/SVM36.png,https://intellipaat.com/mediaFiles/2019/07/SVM37.png,https://intellipaat.com/machine-learning-certification-training-course/,https://intellipaat.com/mediaFiles/2019/07/SVM38.png,https://intellipaat.com/mediaFiles/2019/07/SVM40.png,https://intellipaat.com/mediaFiles/2019/07/SVM39.png,https://intellipaat.com/mediaFiles/2019/07/SVM41.png,https://intellipaat.com/mediaFiles/2019/07/SVM42.png,https://intellipaat.com/all-courses/big-data/?utm_source=salesforce-tutorial-page&utm_medium=Tutorial-Page&utm_campaign=May-all-courses%2Fbig-data%2F,https://intellipaat.com/blog/what-is-random-forest-algorithm-in-python/,https://intellipaat.com/blog/decision-tree-algorithm-in-machine-learning/,"
17,Overview of Deep Learning,https://www.hackerearth.com/blog/developers/artificial-intelligence-101-how-to-get-started/,"From the moment we open our eyes in the morning our brain starts collecting data from different sources. To keep up with the pervasive growth of data from different sources mankind was introduced with modern Data Driven Technologies like Artificial Intelligence, Machine Learning, Deep Learning etc. These technologies have engineered our society in many aspects already and will continue to do so. This tutorial series guides you through the basics of Deep Learning, setting up environment in your system to building the very first Deep Neural Network model.Become Master of Machine Learning by going through this online Machine Learning Course in Hyderabad.Deep Learning is a subset of Machine Learning which is used to achieve Artificial Intelligence. Confusing? Let us look at the diagram given below to have a better understanding of these words.  In other words, Deep Learning is an approach to learning where we can make a machine imitate the network of neurons in a human brain. It consists of algorithms which allow machines to train to perform tasks like speech, image recognition and Natural Language Processing. It is a statistical approach based on Deep Networks, where we break down a task and distribute into machine learning algorithms. These algorithms are constructed with connected layers. In between first layer or input layer and last layer or output layer we have set of hidden layers in between that eventually gave rise to the word Deep which means networks that join neurons in more than two layers. These neurons are connected to one another, which propagates the input signal after it goes through the process. In Deep Learning a network can consume a large amount of input data, then process them through multiple layers because of which we can learn complex features of the data. Now that we have gathered an idea of what Deep Learning is, let’s see why we need Deep Learning.Interested in learning Machine Learning? Click here to learn more in this Machine Learning Training in New York!Our human brain can easily differentiate between a cat and a dog. But how can we make a machine differentiate between a cat and a dog? We would train the machine with a lot of images of cats and dogs. Then once the training is done we will provide the machine with an image of either cat or a dog. Now, we will manually extract some features from the image and make a machine learning model out of it, which would help the machine recognize the input  image. And then the machine learning model will predict whether it was a dog or cat. It was easy, wasn’t it? But what will happen when we have a large number of inputs? Manual extraction of features for a large input is backbreaking work. What if we could skip the manual extraction part? Wouldn’t it make things a lot easier? When the amount of input data is increased, traditional machine learning techniques are insufficient in terms of performance. That is when Deep Learning came into the picture.Deep Learning and its innovations are advancing the future of precision medicine and health management. Breast Cancer, Skin Cancer diagnostics are just a few examples of Deep Learning in Health Care. In coming years computer aided diagnosis will play a major role in healthcare.Describing photos, restoring pixels, restoring colors in B&W photos and videos.Self-driving cars, beating people in computer games, making robots act like human are all possible due to AI and Deep Learning.Apple’s Siri, Google Now, Microsoft Cortana are a few examples of deep learning is voice search & voice-activated intelligent assistants.Deep Learning makes allows and publishers and ad networks to leverage their content to create data-driven predictive advertising, precisely targeted advertising and much more.Predicting natural hazards and seating up a deep-learning-based emergency alert system is to play an important role in coming years.Analyze trading strategy, review commercial loans and form contracts, cyber-attacks are examples of Deep Learning in the Finance Industry.We have both collection and access to the data, we have software’s like TensorFlow which makes building and deploying models easy. That is how Deep Learning is reshaping automation industry in a big way, becoming one of the hottest evolving technologies of 21st century. Which also means that this is the perfect time to acquire this skill. So now that we have learnt the importance and applications of Deep Learning let’s go ahead and see workings of Deep Learning. Also, we will discuss one use case on Deep Learning by the end of this tutorial.If you have any doubts or queries related to Data Science, do post on Machine Learning Community.Before moving ahead with how Deep Learning works, let us try to understand take how biological neural network works.  Our human brain is a neural network, which is full of neurons and each neuron is connected to multiple neurons. Again, neurons have several Dendrites. Dendrites collect input signals which are summed up in the Cell body and later are transmitted to next neuron through Axon. Similarly, in an artificial neural network a perceptron receives multiple inputs which are then processed through functions to get an output. But in case of artificial neural network weights are assigned to various neurons. Then in final layer everything is put together to come up with an answer. Let us compare Biological Neural Network to Artificial Neural Network:A perceptron is an artificial neuron unit in a neural network. It is an algorithm that enables neurons to learn and processes elements in the training set one at a time for supervised learning of binary classifiers that does certain computations to detect features or business intelligence in the input data. There are two types of Perceptrons: Single layer Perceptrons is the simplest type of artificial neural network can learn only linearly separable patterns. This type of perceptron is based on a threshold transfer function.For the best of career growth, check out Intellipaat’s Machine Learning Course and get certified. Neural networks with two or more layers are called multi-layer perceptron. This type of neural network has greater processing power. In this, the algorithm consists of two phases: the forward phase where the activations are propagated from the input to the output layer, and the backward phase, where the error between the observed actual and the requested nominal value in the output layer is propagated backwards to modify the weights and bias values.Deep neural network refers to neural networks with multiple hidden layers and multiple non-linear transformations.As we can see above, simple neural network has only one hidden layer, whereas deep learning neural network has multiple hidden layers. Understanding workings of Deep Learning with an example: Here we are going to take an example of one of the open datasets for Deep Learning every Data Scientists should work on, MNIST- a dataset of handwritten digits. This is one of the most popular deep learning datasets available on the internet.  Go through this Artificial Intelligence Interview Questions And Answers to excel in your Artificial Intelligence Interview. Now, let me ask you a question, what role do the hidden layers play in this process? To understand that let us relate to the biological neural network system and how our brain would recognize a digit from an image. When we see an image of the digit 9, our brain breaks it down as one circle on top. And one line on bottom. Which separately represents 0 and 1. Similarly with 8, one circle on top another on bottom.   Similarly, in deep learning, hidden layers break down the components of the given image forming a pattern. Feed in the image of 9, some specific neurons whose activation would become close to 1.  Combination of these components will trigger a neuron(see the last neuron of the output layer ) with high activation in the last layer. Thus, giving us an output digit.Some of the well-known platforms for Deep Learning:In this tutorial series, we will be focusing on modelling our very first Deep Neural Network using TensorFlow. TensorFlow is one of the best libraries available to implement deep learning. TensorFlow is a software library for numerical computation of mathematical expression. Next part of this tutorial guides you through the basics of TensorFlow and its installation  on your system and how tensor flow helps us implement Deep Learning.  Jump right into the TensorFlow Use Case Tutorial, if TensorFlow is already installed in your system.To get a more elaborate idea with the algorithms of deep learning refer to our Deep Learning Course.  ",,"Overview of Deep Learning,Table of Content:,Watch this Deep Learning Tutorial,What is Deep Learning?,Why do we need Deep Learning?,Applications of Deep Learning:,Watch this Machine Learning and Its Applications Tutorial,Why should you opt for Deep Learning now?,Biological Neural Network vs Artificial Neural Network:,Perceptron:,,Deep Neural Network:,,Deep Learning Platforms:,","https://intellipaat.com/blog/what-is-artificial-intelligence/,https://intellipaat.com/machine-learning-certification-training-course-hyderabad/,#_what_is_deep,#_why_need_deep,#_application_deep_learning,#_why_deep_learning,#_biological_neural_Network,#_perceptron,#_single_layer_perception,#_multi_layer_perception,#_deep_neural_network,#_understanding_deep_learning,#_about_mnist,#_working_explanation,#_deep_learning_platform,https://intellipaat.com/blog/what-is-machine-learning/,https://intellipaat.com/machine-learning-course-in-new-york/,https://intellipaat.com/community/machine-learning,https://intellipaat.com/blog/tutorial/machine-learning-tutorial/neural-network-tutorial/,https://intellipaat.com/machine-learning-certification-training-course/,https://intellipaat.com/blog/interview-question/artificial-intelligence-interview-questions/,https://intellipaat.com/artificial-intelligence-masters-training-course/?utm_source=Machine-Learning-Tutorial&utm_medium=Blog-Tutorial&utm_campaign=may-%2Fartificial-intelligence-masters-training-course%2F,https://intellipaat.com/blog/tutorial/artificial-intelligence-tutorial/tensorflow-andits-installation-on-windows/,https://intellipaat.com/blog/tutorial/artificial-intelligence-tutorial/deep-learning-with-tensorflow-use-case/,https://intellipaat.com/artificial-intelligence-deep-learning-course-with-tensorflow/,https://intellipaat.com/artificial-intelligence-deep-learning-course-with-tensorflow/,"
18,Introduction,https://www.hackerearth.com/blog/developers/artificial-intelligence-101-how-to-get-started/,"Have you ever wondered, how your brain recognizes numbers? No matter how the digits or numbers looks like, brain will relate that to the best possible pattern and concludes the result. This is where the thinking came out to make a something which can recognize similar number patterns, and that is where Neural Networks starts. Let’s discuss a situation The digits in the above picture had been written in extremely low resolution of 28px by 28px even though how effortlessly your brain identifies that as 6. Similarly, there can be very different shapes and pattern for a same number but somehow your visual cortex resolves those as representing the same idea while recognizing the other pictures as their own distinct idea.  In this article, you will learn about the basics of Neural Networks/Artificial Neural Networks and their implementation. Here we have the list of topics if you want to jump right into a specific one:Here you will also learn about the Neural Network basics and its key features.For the best of career growth, check out Intellipaat’s Machine Learning Course and get certified. In the above topic, you learned about the Neural Network. Now there is a question which you might be wondering that what motivates towards Neural Network? Motivation behind neural network is human brain. Human brain is called as the best processor even though it works slower than other computers. Many researchers thought to make a machine that would work in the prospective of the human brain. Human brain contains billion of neurons which are connected to many other neurons to form a network so that if it sees any image, it recognizes the image and processes the output. In the similar manner, it was thought to make artificial interconnected neurons like biological neurons making up an Artificial Neural Network(ANN). Each biological neuron is capable of taking a number of inputs and produce output. Neurons in human brain are capable of making very complex decisions, so this means they run many parallel processes for a particular task. One motivation for ANN is that to work for a particular task identification through many parallel processes.Go through this Artificial Intelligence Interview Questions And Answers to excel in your Artificial Intelligence Interview.Artificial Neuron are also called as perceptrons. This consist of the following basic terms:How perceptron works? A. All the inputs X1, X2, X3,…., Xn multiplies with their respective weights.  B. All the multiplied values are added.  C.  Sum of the values are applied to the activation function.Weight without bias curve graph: Here, by changing weights, you can very input and outputs. Different weights changes the output slope of the activation function. This can be useful to model Input-Output relationships. What if you only want output to be changed when X>1? Here, the role of bias starts. Let’s alter the above example with bias as input. As you can see, by varying the bias b, you can change when the node activates. Without a bias, you cannot vary the output. If you have any doubts or queries related to Data Science, do post on Machine Learning Community. Input Layer Input layer contains inputs and weights. Example: X1, W1, etc. Hidden Layer In a neural network, there can be more than one hidden layer. Hidden layer contains the summation and activation function. Output Layer Output layer consists the set of results generated by the previous layer. It also contains the desired value, i.e. values that are already present in the output layer to check with the values generated by the previous layer. It may be also used to improve the end results. Let’s understand with an example. Suppose you want to go to a food shop. Based on the three factors you will decide whether to go out or not, i.e.Based on the conditions, you choose weight on each condition like W1=6 for money as money is the first important thing you must have, W2=2 for vehicle and W3=2 for weather and say you have set threshold to 5. In this way, perceptron makes decision making model by calculating X1W1, X2W2, and X3W3 and comparing these values to the desired output. Activation functions are used for non-linear complex functional mappings between the inputs and required variable. They introduce non-linear properties to our Network. They convert an input of an artificial neuron to output. That output signal now is used as input in the next layer.  Simply, input between the required values like (0, 1) or (-1, 1) are mapped with the activation function. Why Activation Function? Activation Function helps to solve the complex non-linear model. Without activation function, output signal will just be a linear function and your neural network will not be able to learn complex data such as audio, image, speech, etc. Some commonly used activation functions are:Sigmoid Activation Function:Sigmoid Activation Function can be represented as: f(x) = 1 / 1 + exp(-x)Tanh- Hyperbolic tangentTanh can be represented as: f(x) = 1 — exp(-2x) / 1 + exp(-2x)It solves the problem occurring with Sigmoid function. Output of Tanh is zero centered because range is between -1 and 1. Optimization is easy as compared to Sigmoid function.  But still it suffers gradient vanishing problem. ReLu- Rectified Linear units It can be represented as:  R(x) = max(0,x)   if x < 0 , R(x) = 0 and if x >= 0 , R(x) = xIt avoids as well as rectifies vanishing gradient problem. It has six times better convergence as compared to tanh function.  It should be used within hidden layers of the neural network.Interested in learning Machine Learning? Click here to learn more in this Machine Learning Training in Bangalore!Gradient is the slope of the error curve.  The idea of introducing gradient to reduce the or minimize the error between the desired output and the input. To predict the output based on the every input, weight must be varied to minimize the error. Problem is how to vary weight seeing the output error. This can be solved by gradient descent. In the above graph, blue plot shows the error, red dot shows the ‘w’ value to minimize the error and the black cross or line show the gradient. At point 1, random ‘w’ value is selected with respect to error and gradient is checked. If gradient is positive with respect to the increase in w, then step towards will increase the error and if it is negative with respect to increase in ‘w’, then step towards will decrease the error. In this way, gradient shows the direction of the error curve. The process of minimizing error will continue till the output value reaches close to the desired output. This is a type of Backpropagation. Wnew=Wold–α ∗∇error Wnew= new ‘w’ position Wold= current or old ‘w’ position ∇error= gradient of error at Wold. α= how quickly converges to minimum error Example: Minimum value of the equation f(x)=x4–3x3+2 Minimum value of x is 2.25 by mathematical calculation, so program must give the gradient for that.Guess= input * weight Error= Desired Output – Guess You already know how to minimize the error. Example:  In the above picture, you can see that it is impossible to draw a straight line in case of XOR. So, linear classifier fails in case of Single Layer Perceptron.Become Master of Machine Learning by going through this online Machine Learning course in Singapore.However, multilayer perceptron uses the backpropagation algorithm that can successfully classify the XOR data. This is how backpropagation works. It uses gradient descent algorithm. Let’s see a program that explains the multi-layer propagationRun the above code and you will get the main output as 0.862 which is close to the desired output 1. Small error is acceptable. Mainly used Neural Network are:Images having high pixels cannot be checked under MLP or regular neural network. In CIFAR-10, images are of the size 32*32*3., i.e. 3072 weights. But for image with size 200*200*3, i.e. 120,000 weights, number of neurons required will be more. So, fully connectivity is not so useful in this situation. In CNN, the input consists of images and the layers having neurons in three-dimensional structure, i.e. width, height, depth. Example: In CIFAR-10, the input volume has dimensions, i.e. width, height, depth 32*32*3.  In convolutional layer, neurons receive input from only a restricted subarea of the previous layer, i.e. neurons will be only connected to a small region to the layer before it and not in the fully connecting manners. So, the output layer for CIFAR-10 would have dimensions 1*1*10 because by the end of the CNN the full image will get converted into a small vector along its depth. A simple CNN for CIFAR-10 has the following sections:In this way, CNN transforms the image layer by layer to the class score. It is not important that all the layers contain the same parameters.  CNN Overview:         W2= (W1-F+2P)/S+1          H2=(H1-F+2P)/S+1          D2=KLearn Machine Learning from experts, click here to more in this Machine Learning Training in London!You have learned how to represent a single word. But how could you represent phrases or sentences? Also, can you model relation between words and multi-word expressions? Example: “consider” = “take into account” or can you extract representations of full sentences that preserve some of its semantic meaning? Example: “words representation learned from Intellipaat” = “Intellipaat trained you on text data sets representations” To solve this problem recursive neural network was introduced. It uses binary tree and is trained to identify related phrases or sentences. Example: A wise person suddenly enters the Intellipaat  The idea of recursive neural network is to recursively merge pairs of a representation of smaller segments to get representations uncover bigger segments. The tree structure works on the two following rules:Let’s say a parent has two children  In place of  and , there can be two words from a sentence as seen in the above picture. By checking the scores of each pair it produces the output.  What is RNN? In the above picture, at each time step, in addition to the user’s input, it also accepts the output of the previous hidden layer. Recurrent NN operates on linear chain. So, you can say that Recursive NN operates on hierarchical structure whereas Recurrent NN operates on the chain structures. Let’s see what is Recurrent NN:Let’s say you have a normal neural network to which input is Intellipaat. It processes the word character by character. By the time it reaches to the character ‘e’, it has already forgotten about ‘I’, ‘n’, and ‘t’. So, it is impossible for normal neural network to predict next letter. Recurrent NN remembers that because it has its own internal memory. It produces output, copies that output and loops it back into the network. Let’s take a practical example to understand Recurrent NN: Let’s say you are the perfect person for Intellipaat, but why? Because every week, you attain regular courses from Intellipaat to develop your skills. Your schedules are: Monday = Java, Tuesday = Data Science, Wednesday = Hadoop, Thursday= AWS  If you want a NN to tell you about your next day course then, you have to enter today’s course. So, if you enter input Java then, it should output Data Science. When input is Data Science, then output should be Hadoop, and so on.  From the above example, output is recursively is going to the input to judge the next output.What will happen if there are 100 or more than 100 cases in Recurrent NN? What if there are 150 days in a week and you want NN to tell you next day’s course information?  In the above picture, you have 150 states. Each state has a gradient of 0.01. To update the gradient in the first state it would be   which is almost equals to zero. So, the update in the weight will become zero. In this case, NN will not learn anything to improve and will produce the same error. This is also called as vanishing gradient problem. This is where LSTM(Long-short term Memory) came into action. What is LSTM? Long Short-Term Memory (LSTM) networks are the better version of Recurrent NN that extends Recurrent NN’s memory. LSTM’s memory is like computer’s memory because it can read, write and delete data. In LSTN there are three gates: input gate, forget gate and output gate. Input gate decides whether to let the input pass or not, forget gate deletes the garbage information and output gate outputs the data at the given time step. By providing cell gates mechanism to Recurrent NN, LTSM becomes more efficient than Recurrent NN. Applications of Neural Network As you are now aware of Neural Network, it’s working and types then, let’s know where it can be implemented.There are many more applications of Neural Network which are helpful in our day-to-day life.This brings us to the end of Neural Network tutorial. In this tutorial, we learned in detail about the overview of Neural Network. We also covered almost all the main topics of Neural Network, its programming part, types, motivation, etc. If you want to learn more, I would suggest you to try our Intellipaat “Neural Network” course that covers in-depth knowledge of most of the important topics like how to train a model, how to program for that and different approaches towards Neural Network. Join Intellipaat Artificial Intelligence & Deep Learning Course with Tensorflow today and let your career boom towards the domain of Big Data and higher opportunities.",,"Introduction,Artificial Neural Network,Watch this Neural Network Tutorial Tutorial Video,Structure of Neural Network,Types of Neural Network,Watch this Convolutional Neural Network Tutorial (CNN) Video,Conclusion,","#_Artificial_Neural_Network,#_What_is_Artificial,#_Biological_Motivation,#_Structure_of_Neural,#_Artificial_Neuron,#_Weights_and_Bias,#_Layers,#_Activation_Function,#_Gradient_Descent,#_Feed_Neural_Network,#_Single_Layer_Perceptron,#_Multi_Layer_Perceptron,#_Types_of_Neural,#_C_Neural_Network,#_Recur_Neural_Network,#_Rt_Neural_Network,#_Long_Memory,https://intellipaat.com/machine-learning-certification-training-course/,https://intellipaat.com/blog/interview-question/artificial-intelligence-interview-questions/,https://intellipaat.com/community/machine-learning,https://intellipaat.com/artificial-intelligence-masters-training-course/?utm_source=Machine-Learning-Tutorial&utm_medium=Blog-Tutorial&utm_campaign=may-%2Fartificial-intelligence-masters-training-course%2F,https://intellipaat.com/machine-learning-certification-training-course-bangalore/,https://intellipaat.com/all-courses/big-data/?utm_source=salesforce-tutorial-page&utm_medium=Tutorial-Page&utm_campaign=May-all-courses%2Fbig-data%2F,https://intellipaat.com/machine-learning-course-in-singapore/,https://intellipaat.com/machine-learning-certification-training-course/?utm_source=Machine-Learning-Tutorial&utm_medium=Blog-Tutorial&utm_campaign=may-%2Fmachine-learning-certification-training-course%2F,https://intellipaat.com/machine-learning-course-in-london/,https://intellipaat.com/artificial-intelligence-masters-training-course/?utm_source=Machine-Learning-Tutorial&utm_medium=Blog-Tutorial&utm_campaign=may-%2Fartificial-intelligence-masters-training-course%2F,https://intellipaat.com/all-courses/big-data/?utm_source=salesforce-tutorial-page&utm_medium=Tutorial-Page&utm_campaign=May-all-courses%2Fbig-data%2F,https://intellipaat.com/artificial-intelligence-deep-learning-course-with-tensorflow/,"
19,TensorFlow – The Machine Learning Library,https://www.hackerearth.com/blog/developers/artificial-intelligence-101-how-to-get-started/,"Machine learning is eating the software industry and Deep learning is eating machine learning. Google developed this open source software library for the implementation of Machine Learning and Deep Neural Network research. Big IT firms like Facebook, Microsoft, and Google are already implementing and targeting to excel in Deep Learning.In our previous tutorial we have discussed how Deep learning is reshaping the world of technology. Here we are going to introduce TensorFlow and its installation steps, Let’s get started.TensorFlow is a Deep Learning toolkit with low-level functionality yet high-level operations, designed for Dataflow programming models. This tool is not only less time consuming both portable and scalable on a lot of platforms, which means the code can run on CPU, GPU (Graphical Processing Units), mobile devices and TPU (Tensor Processing Units, which are Google’s dedicated TensorFlow processors).  This effective software framework uses Python as main interface. It is also backed up by a huge community of developers, also because of the early adoption by academic and industrial research teams across the world makes TensorFlow very popular for Deep Learning. Above all, TensorFlow has the power of Google behind it. TensorFlow and its functions revolve primarily around Tensor, which means multidimensional array and Flow, which refers to graphs.The tensor data flows through the graph while being operated on at the nodes. TensorFlow describes multi-dimensional numerical array as graphs without using complex mathematical interpretations, which makes model analysis very easy.  An n-dimensional array, Scalar, Vector, Matrix these are all Tensors.  Tensors are considered as data structure in TensorFlow. Rank: Rank of 1D array =0 Rank of 2D array =1 Rank of 3D array =2For the best of career growth, check out Intellipaat’s Machine Learning Course and get certified. A Flow Graph is a directed graph with, where nodes represent mathematical operations and edges represent flow of data as tensors, where we have learnt how data can be multi-dimensional. We can have a better idea about tensors and flow graphs from the image shown below.Let us take an example, here we are representing flow graph for the numerical calculation ofd=(ab+c)2  So, in a TensorFlow data basically flows in the form of tensors.Go through this Artificial Intelligence Interview Questions And Answers to excel in your Artificial Intelligence Interview. Now that we have learnt that TensorFlow is, let us discuss why do we choose TensorFlow over any other tool available. Deep learning uses algorithms known as Neural Networks, which have been designed to imitate the biological neurons. Using these Artificial Neural Networks, the computer is able to learn from data, represent it and identify solutions based on the data fed into the system.  And TensorFlow lets us build large-scale neural networks with multiple hidden layers.If you have any doubts or queries related to Data Science, do post on Machine Learning Community.ProsConsTensorFlow is designed to support experimentation with new machine learning models and system-level optimization. TensorFlow architecture has three primary steps.Interested in learning Machine Learning? Click here to learn more in this Machine Learning Training in New York!Constant are created by tf.constatnt() function. Vale, dtype, shape, name, verify_shape are the arguments that can be passed to a constant.TensorFlow is a way of representing computation without actually performing it until asked. In this sense, it is a form of lazy computing, and it allows for some great improvements to the running of code.global_vaiables_initializer is used for initializing the variables globally i.e. the variables can be used in any part of the code.Placeholders allow you to assign data after creating your operation and adding computation graph. In TensorFlow terminology we can feed data into the graph through these placeholders.Sessions are used to evaluate tensors and it runs TensorFlow operations. It encapsulated the state of a TensorFlow runtime. Some example of TensorFlow session areWhen you ask for the Session.run output of a node then TensorFlow go through the graph and runs through all the node that gives the input to the requested output node. So that way  you will be able to print the expected value which is Hello World. The first thing to notice while installing TensorFlow is to choose either CPU or GPU supported version. For beginners I would recommend you use CPU supported version if you need to train simple machine learning models. GPU supported TensorFlow requires you to install a number of libraries and drivers. It supports NVIDIA GPU card, with support for CUDA Compute 3.5 or higher.Become Master of Machine Learning by going through this online Machine Learning course in Sydney.I would recommend you install TensorFlow through virtual environment because it is very useful when you change your environment or platform.As we discussed TensorFlow is very powerful framework and has grown in popularity and is now being used by developers for solving problems using deep learning methods for image recognition, video detection, text processing like sentiment analysis, etc. It takes some time to get used to TensorFlow but once you’re clear with the concepts you can master TensorFlow and play around it. In the next part of this tutorial we will build our very first Deep Neural Network model and how tensor flow helps us implement Deep Learning.To get a more elaborate idea with the algorithms of deep learning refer to our Deep Learning Course.",,"TensorFlow – The Machine Learning Library,Watch this Tensorflow Tutorial for Beginners Video,Content:,What is TensorFlow?,Basic TensorFlow Code Structure,Why TensorFlow?,How TensorFlow works,TensorFlow installation steps in Windows,Conclusion,","https://intellipaat.com/blog/what-is-machine-learning/,https://intellipaat.com/blog/tutorial/machine-learning-tutorial/neural-network-tutorial/,https://intellipaat.com/blog/tutorial/artificial-intelligence-tutorial/introduction-deep-learning/,#_what_is_tensorflow,#_what_is_tensor,#_what_is_flow_graph,#_tensorFlow_graph_numeric,#_basic_tensorflow_code,#_why_tensor_flow,#_app_of_tensorflow,#_feature_of_tensorflow,#_pros_cons_tensorflow,#_how_does_tensor,#_tensor_flow_archi,#_preprocessing_the_data,#_build_the_model,#_train_estimate_model,#_tensorflow_the_components,#_tensorflow_install_steps,https://intellipaat.com/machine-learning-certification-training-course/?utm_source=Machine-Learning-Tutorial&utm_medium=Blog-Tutorial&utm_campaign=may-%2Fmachine-learning-certification-training-course%2F,https://intellipaat.com/machine-learning-certification-training-course/,https://intellipaat.com/blog/interview-question/artificial-intelligence-interview-questions/,https://intellipaat.com/artificial-intelligence-masters-training-course/?utm_source=Machine-Learning-Tutorial&utm_medium=Blog-Tutorial&utm_campaign=may-%2Fartificial-intelligence-masters-training-course%2F,https://intellipaat.com/community/machine-learning,https://intellipaat.com/all-courses/big-data/?utm_source=salesforce-tutorial-page&utm_medium=Tutorial-Page&utm_campaign=May-all-courses%2Fbig-data%2F,https://intellipaat.com/machine-learning-course-in-new-york/,https://intellipaat.com/machine-learning-certification-training-course/?utm_source=Machine-Learning-Tutorial&utm_medium=Blog-Tutorial&utm_campaign=may-%2Fmachine-learning-certification-training-course%2F,https://intellipaat.com/machine-learning-course-in-sydney/,https://intellipaat.com/artificial-intelligence-masters-training-course/?utm_source=Machine-Learning-Tutorial&utm_medium=Blog-Tutorial&utm_campaign=may-%2Fartificial-intelligence-masters-training-course%2F,https://intellipaat.com/blog/tutorial/artificial-intelligence-tutorial/deep-learning-with-tensorflow-use-case/,https://intellipaat.com/artificial-intelligence-deep-learning-course-with-tensorflow/,"
